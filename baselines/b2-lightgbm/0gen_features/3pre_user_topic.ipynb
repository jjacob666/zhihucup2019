{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../../data'\n",
    "features_path = '../1pre_features'\n",
    "answer_info = pd.read_csv(os.path.join(data_path, 'answer_info_0926.txt'), header=None, sep='\\t')\n",
    "#train1 = pd.read_csv(os.path.join(data_path, 'invite_info_0926.txt'), header=None, sep='\\t')\n",
    "\n",
    "#train1.columns = ['问题id', '用户id', '邀请创建时间','是否回答']\n",
    "answer_info.columns = ['回答id', '问题id', '用户id', '回答创建时间', '回答内容的单字编码序列', '回答内容的切词编码序列', '回答是否被标优', '回答是否被推荐', '回答是否被收入圆桌', '是否包含图片', '是否包含视频', '回答字数', '点赞数', '取赞数', '评论数', '收藏数', '感谢数', '举报数', '没有帮助数', '反对数']\n",
    "\n",
    "drop_feat = ['回答内容的单字编码序列', '回答内容的切词编码序列', '回答是否被标优', '回答是否被推荐', '回答是否被收入圆桌', '是否包含图片', '是否包含视频',]\n",
    "answer_info  = answer_info.drop(drop_feat, axis=1)\n",
    "\n",
    "#user_answer_info = pd.merge(train1, answer_info, how='left', on='用户id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_info['回答创建时间-hour'] = answer_info['回答创建时间'].apply(lambda x:x.split('-')[1].split('H')[1]).astype(int)\n",
    "answer_info['回答创建时间-day'] = answer_info['回答创建时间'].apply(lambda x:x.split('-')[0].split('D')[1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trainuser_topic.pkl','rb') as file:\n",
    "    train_user_answered_topics = pickle.load(file)\n",
    "print(train_user_answered_topics.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_info_train = pd.merge(answer_info, train_user_answered_topics, left_on='用户id',right_on='uid', how='left').drop(['uid'], axis=1)\n",
    "print(answer_info_train.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del answer_info_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('valuser_topic.pkl','rb') as file:\n",
    "    test_user_answered_topics = pickle.load(file)\n",
    "print(test_user_answered_topics.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_info_test = pd.merge(answer_info, test_user_answered_topics, left_on='用户id',right_on='uid', how='left').drop(['uid'], axis=1)\n",
    "print(answer_info_test.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del answer_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 减少内存占用\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        df[col] = df[col].fillna(0)\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int' or str(col_type)[:5] == 'float':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            \n",
    "#             if str(col_type)[:5] == 'float':\n",
    "#                 df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 减少内存占用\n",
    "def reduce_mem_usage111(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        df[col] = df[col].fillna(0)\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':# or str(col_type)[:5] == 'float':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            \n",
    "            if str(col_type)[:5] == 'float':\n",
    "                df[col] = df[col].astype(np.float16)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('traintopic_count.pkl','rb') as file:\n",
    "    topic_count_train = pickle.load(file)\n",
    "print(train_topic_count.head())\n",
    "\n",
    "with open('valtopic_count.pkl','rb') as file:\n",
    "    topic_count_test = pickle.load(file)\n",
    "print(test_topic_count.head())\n",
    "\n",
    "train_u_topic_count = pd.merge(train_user_answered_topics, topic_count_train, left_on='u_t_answered_topics', right_on='user_topic_id', how='left').drop(['user_topic_id'], axis=1)\n",
    "test_u_topic_count = pd.merge(test_user_answered_topics, topic_count_test, left_on='u_t_answered_topics', right_on='user_topic_id', how='left').drop(['user_topic_id'], axis=1)\n",
    "print(train_u_topic_count.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_u_topic_count.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_u_topic(u_topic_info, etype, func):\n",
    "    question_label = u_topic_info.copy()\n",
    "    question_label['topic_'+func] = question_label['topic_'+func].fillna(0)\n",
    "    question_label['topic_'+func] = question_label['topic_'+func].astype(int)\n",
    "    question_label['u_t_a_'+func+'_sum'] = question_label['uid'].map(question_label.groupby('uid')['topic_'+func].agg('sum').astype(int))\n",
    "    question_label['u_t_a_'+func+'_mean'] = question_label['uid'].map(question_label.groupby('uid')['topic_'+func].agg('mean'))\n",
    "    question_label['u_t_a_'+func+'_max'] = question_label['uid'].map(question_label.groupby('uid')['topic_'+func].agg('max').astype(int))\n",
    "    question_label['u_t_a_'+func+'_std'] = question_label['uid'].map(question_label.groupby('uid')['topic_'+func].agg('std'))\n",
    "    question_label['u_t_a_'+func+'_min'] = question_label['uid'].map(question_label.groupby('uid')['topic_'+func].agg('min').astype(int))\n",
    "    \n",
    "    del question_label['topic_'+func], question_label['u_t_answered_topics']\n",
    "    question_label = question_label.drop_duplicates(inplace=False)\n",
    "    \n",
    "    print(question_label.info())\n",
    "    \n",
    "    with open('for_ut/'+etype+'user_topic_'+func+'.pkl', 'wb') as file:\n",
    "        pickle.dump(question_label, file)\n",
    "    \n",
    "    return question_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_u_topic_count_stats = merge_u_topic(train_u_topic_count, 'train', 'count')\n",
    "test_u_topic_count_stats = merge_u_topic(test_u_topic_count, 'val', 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_t_count(func, train_user_answered_topics, test_user_answered_topics):\n",
    "\n",
    "    with open('traintopic_'+func+'_count.pkl','rb') as file:\n",
    "        topic_count_train = pickle.load(file)\n",
    "    print(topic_count_train.head())\n",
    "\n",
    "    with open('valtopic_'+func+'_count.pkl','rb') as file:\n",
    "        topic_count_test = pickle.load(file)\n",
    "    print(topic_count_test.head())\n",
    "\n",
    "    train_u_topic_count = pd.merge(train_user_answered_topics, topic_count_train, left_on='u_t_answered_topics', right_on='user_topic_id', how='left').drop(['user_topic_id'], axis=1)\n",
    "    test_u_topic_count = pd.merge(test_user_answered_topics, topic_count_test, left_on='u_t_answered_topics', right_on='user_topic_id', how='left').drop(['user_topic_id'], axis=1)\n",
    "    \n",
    "    print(train_u_topic_count.head())\n",
    "    return train_u_topic_count, test_u_topic_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('traintopic_inv_count.pkl','rb') as file:\n",
    "    topic_count_inv_train = pickle.load(file)\n",
    "print(train_topic_count.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_u_topic_good_count, test_u_topic_good_count = get_t_count('good', train_user_answered_topics, test_user_answered_topics)\n",
    "train_u_topic_good_stats = merge_u_topic(train_u_topic_good_count, 'train', 'good_count')\n",
    "test_u_topic_good_stats = merge_u_topic(test_u_topic_good_count, 'val', 'good_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_u_topic_mark_count, test_u_topic_mark_count = get_t_count('mark', train_user_answered_topics, test_user_answered_topics)\n",
    "train_u_topic_mark_stats = merge_u_topic(train_u_topic_mark_count, 'train', 'mark_count')\n",
    "test_u_topic_mark_stats = merge_u_topic(test_u_topic_mark_count, 'val', 'mark_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_u_topic_thank_count, test_u_topic_thank_count = get_t_count('thank', train_user_answered_topics, test_user_answered_topics)\n",
    "train_u_topic_thank_stats = merge_u_topic(train_u_topic_thank_count, 'train', 'thank_count')\n",
    "test_u_topic_thank_stats = merge_u_topic(test_u_topic_thank_count, 'val', 'thank_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inv_t_count(func, train_user_answered_topics, test_user_answered_topics):\n",
    "\n",
    "    with open('traintopic_'+func+'_count.pkl','rb') as file:\n",
    "        topic_count_train = pickle.load(file)\n",
    "    print(topic_count_train.head())\n",
    "\n",
    "    with open('valtopic_'+func+'_count.pkl','rb') as file:\n",
    "        topic_count_test = pickle.load(file)\n",
    "    print(topic_count_test.head())\n",
    "\n",
    "    train_u_topic_count = pd.merge(train_user_answered_topics, topic_count_train, left_on='u_t_answered_topics', right_on='user_inv_topic_id', how='left').drop(['user_inv_topic_id'], axis=1)\n",
    "    test_u_topic_count = pd.merge(test_user_answered_topics, topic_count_test, left_on='u_t_answered_topics', right_on='user_inv_topic_id', how='left').drop(['user_inv_topic_id'], axis=1)\n",
    "    \n",
    "    print(train_u_topic_count.head())\n",
    "    return train_u_topic_count, test_u_topic_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_u_topic_inv_count.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_u_topic_inv_count, test_u_topic_inv_count = get_inv_t_count('inv', train_user_answered_topics, test_user_answered_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_u_inv_topic(u_topic_info, etype, func):\n",
    "    question_label = u_topic_info.copy()\n",
    "    question_label['topic_'+func] = question_label['topic_'+func].fillna(0)\n",
    "    question_label['topic_'+func] = question_label['topic_'+func].astype(int)\n",
    "    question_label['u_t_a_'+func+'_sum'] = question_label['uid'].map(question_label.groupby('uid')['topic_'+func].agg('sum'))\n",
    "    question_label['u_t_a_'+func+'_mean'] = question_label['uid'].map(question_label.groupby('uid')['topic_'+func].agg('mean'))\n",
    "    question_label['u_t_a_'+func+'_max'] = question_label['uid'].map(question_label.groupby('uid')['topic_'+func].agg('max'))\n",
    "    question_label['u_t_a_'+func+'_std'] = question_label['uid'].map(question_label.groupby('uid')['topic_'+func].agg('std'))\n",
    "    question_label['u_t_a_'+func+'_min'] = question_label['uid'].map(question_label.groupby('uid')['topic_'+func].agg('min'))\n",
    "    \n",
    "    del question_label['topic_'+func], question_label['u_t_answered_topics']\n",
    "    question_label = question_label.drop_duplicates(inplace=False)\n",
    "    \n",
    "    print(question_label.info())\n",
    "    \n",
    "    with open('for_ut/'+etype+'user_topic_'+func+'.pkl', 'wb') as file:\n",
    "        pickle.dump(question_label, file)\n",
    "    \n",
    "    return question_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_u_topic_inv_stats1 = merge_u_inv_topic(train_u_topic_inv_count, 'train', 'inv_count')\n",
    "test_u_topic_inv_stats1 = merge_u_inv_topic(test_u_topic_inv_count, 'val', 'inv_count')\n",
    "\n",
    "train_u_topic_inv_stats2 = merge_u_inv_topic(train_u_topic_inv_count, 'train', 'inv_std')\n",
    "test_u_topic_inv_stats2 = merge_u_inv_topic(test_u_topic_inv_count, 'val', 'inv_std')\n",
    "\n",
    "train_u_topic_inv_stats3 = merge_u_inv_topic(train_u_topic_inv_count, 'train', 'inv_sum')\n",
    "test_u_topic_inv_stats3 = merge_u_inv_topic(test_u_topic_inv_count, 'val', 'inv_sum')\n",
    "\n",
    "train_u_topic_inv_stats4 = merge_u_inv_topic(train_u_topic_inv_count, 'train', 'inv_pos_mean')\n",
    "test_u_topic_inv_stats4 = merge_u_inv_topic(test_u_topic_inv_count, 'val', 'inv_pos_mean')\n",
    "\n",
    "train_u_topic_inv_stats5 = merge_u_inv_topic(train_u_topic_inv_count, 'train', 'inv_neg_mean')\n",
    "test_u_topic_inv_stats5 = merge_u_inv_topic(test_u_topic_inv_count, 'val', 'inv_neg_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_u_topic_inv_stats4[test_u_topic_inv_stats4['u_t_a_inv_pos_mean_min']!=0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -----------总结-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 离线加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('for_ut/trainuser_topic_count.pkl','rb') as file:\n",
    "    train_u_topic_count_stats = pickle.load(file)\n",
    "    \n",
    "with open('for_ut/trainuser_topic_good_count.pkl','rb') as file:\n",
    "    train_u_topic_good_stats = pickle.load(file)\n",
    "    \n",
    "with open('for_ut/trainuser_topic_mark_count.pkl','rb') as file:\n",
    "    train_u_topic_mark_stats = pickle.load(file)\n",
    "    \n",
    "with open('for_ut/trainuser_topic_thank_count.pkl','rb') as file:\n",
    "    train_u_topic_thank_stats = pickle.load(file)\n",
    "    \n",
    "with open('for_ut/trainuser_topic_inv_pos_mean.pkl','rb') as file:\n",
    "    train_u_topic_inv_stats4 = pickle.load(file)\n",
    "    \n",
    "with open('for_ut/trainuser_topic_inv_sum.pkl','rb') as file:\n",
    "    train_u_topic_inv_stats3 = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_u_topic_count_stats.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_u_topic_inv_stats4.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('for_ut/valuser_topic_count.pkl','rb') as file:\n",
    "    test_u_topic_count_stats = pickle.load(file)\n",
    "    \n",
    "with open('for_ut/valuser_topic_good_count.pkl','rb') as file:\n",
    "    test_u_topic_good_stats = pickle.load(file)\n",
    "    \n",
    "with open('for_ut/valuser_topic_mark_count.pkl','rb') as file:\n",
    "    test_u_topic_mark_stats = pickle.load(file)\n",
    "    \n",
    "with open('for_ut/valuser_topic_thank_count.pkl','rb') as file:\n",
    "    test_u_topic_thank_stats = pickle.load(file)\n",
    "    \n",
    "with open('for_ut/valuser_topic_inv_pos_mean.pkl','rb') as file:\n",
    "    test_u_topic_inv_stats4 = pickle.load(file)\n",
    "    \n",
    "with open('for_ut/valuser_topic_inv_sum.pkl','rb') as file:\n",
    "    test_u_topic_inv_stats3 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_u_topic_count_stats = reduce_mem_usage(train_u_topic_count_stats)\n",
    "train_u_topic_good_stats = reduce_mem_usage(train_u_topic_good_stats)\n",
    "train_u_topic_thank_stats = reduce_mem_usage(train_u_topic_thank_stats)\n",
    "train_u_topic_mark_stats = reduce_mem_usage(train_u_topic_mark_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_u_topic_inv_stats1 = reduce_mem_usage(train_u_topic_inv_stats1)\n",
    "#train_u_topic_inv_stats2 = reduce_mem_usage(train_u_topic_inv_stats2)\n",
    "train_u_topic_inv_stats3 = reduce_mem_usage111(train_u_topic_inv_stats3)\n",
    "train_u_topic_inv_stats4 = reduce_mem_usage111(train_u_topic_inv_stats4)\n",
    "#train_u_topic_inv_stats5 = reduce_mem_usage(train_u_topic_inv_stats5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(train_u_a_topic.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_u_a_topic = pd.merge(train_u_topic_count_stats, train_u_topic_good_stats, on='uid', how='left')\n",
    "#train_u_a_topic = pd.merge(train_u_a_topic, train_u_topic_good_stats, on='uid', how='outer')\n",
    "train_u_a_topic = pd.merge(train_u_a_topic, train_u_topic_mark_stats, on='uid', how='left')\n",
    "train_u_a_topic = pd.merge(train_u_a_topic, train_u_topic_thank_stats, on='uid', how='left')\n",
    "#train_u_a_topic = pd.merge(train_u_a_topic, train_u_topic_inv_stats1, on='uid', how='left)\n",
    "#train_u_a_topic = pd.merge(train_u_a_topic, train_u_topic_inv_stats2, on='uid', how='left')\n",
    "#train_u_a_topic = pd.merge(train_u_a_topic, train_u_topic_inv_stats3, on='uid', how='left')\n",
    "#train_u_a_topic = pd.merge(train_u_a_topic, train_u_topic_inv_stats4, on='uid', how='left')\n",
    "#train_u_a_topic = pd.merge(train_u_a_topic, train_u_topic_inv_stats5, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_u_a_topic = pd.merge(train_u_a_topic, train_u_topic_inv_stats4, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_u_a_topic = train_u_a_topic.drop(['topic_inv_neg_mean','topic_inv_sum','topic_inv_std','topic_inv_count'], axis=1,  inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_u_a_topic.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_u_topic_count_stats = reduce_mem_usage(test_u_topic_count_stats)\n",
    "test_u_topic_good_stats = reduce_mem_usage(test_u_topic_good_stats)\n",
    "test_u_topic_thank_stats = reduce_mem_usage(test_u_topic_thank_stats)\n",
    "test_u_topic_mark_stats = reduce_mem_usage(test_u_topic_mark_stats)\n",
    "#test_u_topic_inv_stats1 = reduce_mem_usage(test_u_topic_inv_stats1)\n",
    "#test_u_topic_inv_stats2 = reduce_mem_usage(test_u_topic_inv_stats2)\n",
    "#test_u_topic_inv_stats3 = reduce_mem_usage(test_u_topic_inv_stats3)\n",
    "#test_u_topic_inv_stats4 = reduce_mem_usage(test_u_topic_inv_stats4)\n",
    "#test_u_topic_inv_stats5 = reduce_mem_usage(test_u_topic_inv_stats5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_u_topic_inv_stats3 = reduce_mem_usage111(test_u_topic_inv_stats3)\n",
    "test_u_topic_inv_stats4 = reduce_mem_usage111(test_u_topic_inv_stats4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_u_a_topic = pd.merge(test_u_topic_count_stats, test_u_topic_good_stats, on='uid', how='outer')\n",
    "#test_u_a_topic = pd.merge(test_u_a_topic, test_u_topic_good_stats, on='uid', how='outer')\n",
    "test_u_a_topic = pd.merge(test_u_a_topic, test_u_topic_mark_stats, on='uid', how='outer')\n",
    "test_u_a_topic = pd.merge(test_u_a_topic, test_u_topic_thank_stats, on='uid', how='outer')\n",
    "#test_u_a_topic = pd.merge(test_u_a_topic, test_u_topic_inv_stats1, on='uid', how='outer')\n",
    "#test_u_a_topic = pd.merge(test_u_a_topic, test_u_topic_inv_stats2, on='uid', how='outer')\n",
    "#test_u_a_topic = pd.merge(test_u_a_topic, test_u_topic_inv_stats3, on='uid', how='outer')\n",
    "test_u_a_topic = pd.merge(test_u_a_topic, test_u_topic_inv_stats4, on='uid', how='outer')\n",
    "#test_u_a_topic = pd.merge(test_u_a_topic, test_u_topic_inv_stats5, on='uid', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_u_a_topic = test_u_a_topic.drop(['topic_inv_neg_mean','topic_inv_sum','topic_inv_std','topic_inv_count'], axis=1,  inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用户和话题基本的交叉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用户回答过的话题个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_user_answered_topics.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_answered_topics['u_t_a_num'] = train_user_answered_topics['uid'].map(train_user_answered_topics['uid'].value_counts().astype(int))\n",
    "test_user_answered_topics['u_t_a_num'] = test_user_answered_topics['uid'].map(test_user_answered_topics['uid'].value_counts().astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_u_topic_num = train_user_answered_topics[['uid','u_t_a_num']].drop_duplicates(inplace=False)\n",
    "test_u_topic_num = test_user_answered_topics[['uid','u_t_a_num']].drop_duplicates(inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_u_a_topic = pd.merge(train_u_a_topic, train_u_topic_num, on='uid', how='left')\n",
    "test_u_a_topic = pd.merge(test_u_a_topic, test_u_topic_num, on='uid', how='left').drop_duplicates(inplace=False).info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_u_a_topic = train_u_a_topic.drop_duplicates(inplace=False)\n",
    "test_u_a_topic = test_u_a_topic.drop_duplicates(inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('for_ut/user_topic_all_count_train.pkl', 'wb') as file:\n",
    "    pickle.dump(train_u_a_topic, file)\n",
    "\n",
    "with open('for_ut/user_topic_all_count_test.pkl', 'wb') as file:\n",
    "    pickle.dump(test_u_a_topic, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_u_a_topic.drop_duplicates(inplace=False).info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用户回答过的问题个数除以回答过的话题个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
