{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../../data'\n",
    "features_path = '../1pre_features'\n",
    "answer_info = pd.read_csv(os.path.join(data_path, 'answer_info_0926.txt'), header=None, sep='\\t')\n",
    "#train1 = pd.read_csv(os.path.join(data_path, 'invite_info_0926.txt'), header=None, sep='\\t')\n",
    "\n",
    "#train1.columns = ['问题id', '用户id', '邀请创建时间','是否回答']\n",
    "answer_info.columns = ['回答id', '问题id', '用户id', '回答创建时间', '回答内容的单字编码序列', '回答内容的切词编码序列', '回答是否被标优', '回答是否被推荐', '回答是否被收入圆桌', '是否包含图片', '是否包含视频', '回答字数', '点赞数', '取赞数', '评论数', '收藏数', '感谢数', '举报数', '没有帮助数', '反对数']\n",
    "\n",
    "drop_feat = ['回答内容的单字编码序列', '回答内容的切词编码序列', '回答是否被标优', '回答是否被推荐', '回答是否被收入圆桌', '是否包含图片', '是否包含视频',]\n",
    "answer_info  = answer_info.drop(drop_feat, axis=1)\n",
    "\n",
    "#user_answer_info = pd.merge(train1, answer_info, how='left', on='用户id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_info['回答创建时间-hour'] = answer_info['回答创建时间'].apply(lambda x:x.split('-')[1].split('H')[1]).astype(int)\n",
    "answer_info['回答创建时间-day'] = answer_info['回答创建时间'].apply(lambda x:x.split('-')[0].split('D')[1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_info = pd.read_csv(os.path.join(data_path, 'question_info_0926.txt'), header=None, sep='\\t')\n",
    "question_info.columns = ['问题id','问题创建时间','问题标题单字编码','问题标题切词编码','问题描述单字编码','问题描述切词编码','问题绑定话题']\n",
    "drop_feat = ['问题创建时间','问题标题单字编码','问题标题切词编码','问题描述单字编码','问题描述切词编码',]\n",
    "question_info  = question_info.drop(drop_feat, axis=1)\n",
    "\n",
    "answer_qestion_info = pd.merge(answer_info, question_info, on='问题id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成问题的几个补充特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3735352 entries, 0 to 3735351\n",
      "Data columns (total 6 columns):\n",
      "qid              object\n",
      "day              int32\n",
      "hour             int32\n",
      "q_title_num      int64\n",
      "diff_qi_days     int64\n",
      "diff_qi_hours    int64\n",
      "dtypes: int32(2), int64(3), object(1)\n",
      "memory usage: 171.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "question_info1 = pd.read_csv(os.path.join(data_path, 'question_info_0926.txt'), header=None, sep='\\t')\n",
    "question_info1.columns = ['问题id','问题创建时间','问题标题单字编码','问题标题切词编码','问题描述单字编码','问题描述切词编码','问题绑定话题']\n",
    "drop_feat = ['问题标题单字编码','问题描述单字编码','问题绑定话题']\n",
    "question_info1  = question_info1.drop(drop_feat, axis=1)\n",
    "print(question_info1.info())\n",
    "\n",
    "question_info1['q_day'] = question_info1['问题创建时间'].apply(lambda x:x.split('-')[0].split('D')[1]).astype(int)\n",
    "question_info1['q_hour'] = question_info1['问题创建时间'].apply(lambda x:x.split('-')[1].split('H')[1]).astype(int)\n",
    "question_info1['q_title_num'] = question_info1['问题标题切词编码'].apply(lambda x:len(x)).astype(int)\n",
    "drop_feat = ['问题标题切词编码','问题描述切词编码', '问题创建时间']\n",
    "question_info1  = question_info1.drop(drop_feat, axis=1)\n",
    "print(question_info1.head(5))\n",
    "\n",
    "question_info1.to_hdf('question_info1.hd5', key='data')\n",
    "\n",
    "train_test_data = pd.read_hdf(os.path.join(features_path,'data0.h5'), key='data')\n",
    "train_test_data = train_test_data[['qid', 'day', 'hour']]\n",
    "train_test_data = pd.merge(train_test_data, question_info1, left_on='qid', right_on='问题id', how='left').drop('问题id', axis=1)\n",
    "print(train_test_data.head(5))\n",
    "\n",
    "train_test_data['diff_qi_days'] = train_test_data['day'] - train_test_data['q_day']\n",
    "train_test_data['diff_qi_hours'] = train_test_data['hour'] - train_test_data['q_hour']\n",
    "train_test_data.loc[:,'diff_qi_hours'] += 23\n",
    "drop_feat = ['q_day','q_hour']\n",
    "train_test_data  = train_test_data.drop(drop_feat, axis=1)\n",
    "print(train_test_data.head(5))\n",
    "\n",
    "print(train_test_data.info())\n",
    "\n",
    "train_test_data.to_hdf('q_ex_features.h5', key='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---分割线---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          回答id         用户id         问题id                       问题绑定话题  回答字数  \\\n",
      "0  A2502060945   M625498202  Q1867533817  T381,T8211,T3144,T4936,T823    41   \n",
      "1  A2847829478   M142330444  Q3366788616           T5490,T2180,T17098   204   \n",
      "2  A2005999231   M771499642  Q4264694221           T8716,T10196,T8767    54   \n",
      "3    A14821523  M2282072267  Q1088851650                         T258    42   \n",
      "4   A731550034  M2282072267  Q1023877868                     T95,T545    44   \n",
      "\n",
      "   点赞数  感谢数  收藏数  回答创建时间-day  \n",
      "0    1    1    0        3808  \n",
      "1    1    1    3        3810  \n",
      "2    2    0    0        3853  \n",
      "3    1    1    0        3859  \n",
      "4    0    0    0        3855  \n"
     ]
    }
   ],
   "source": [
    "answer_qestion_info  = answer_qestion_info[['回答id','用户id','问题id','问题绑定话题','回答字数','点赞数','感谢数','收藏数','回答创建时间-day']]\n",
    "print(answer_qestion_info.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 减少内存占用\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int' or str(col_type)[:5] == 'float':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            \n",
    "            if str(col_type)[:5] == 'float':\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def creat_2pkl(answer_qestion_info, train_label, etype):\n",
    "    answer_qestion_info1 = answer_qestion_info['问题绑定话题'].str.split(',', expand=True).stack()\n",
    "    #print(answer_qestion_info1)\n",
    "    answer_qestion_info1 = answer_qestion_info1.reset_index(level=1, drop=True).rename('user_topic_id')\n",
    "    answer_qestion_info = answer_qestion_info.join(answer_qestion_info1)\n",
    "    print(answer_qestion_info.head(10))\n",
    "    print(answer_qestion_info.info())\n",
    "    del answer_qestion_info1\n",
    "    answer_qestion_info = reduce_mem_usage(answer_qestion_info)\n",
    "    \n",
    "    user_topic_word = answer_qestion_info[['用户id','user_topic_id','回答字数']]\n",
    "    t1 = answer_qestion_info.groupby(['用户id','user_topic_id'])['回答字数'].agg('sum').to_frame()\n",
    "    t1.columns = ['u_topic_word_count']\n",
    "    t1 = t1.reset_index()\n",
    "    user_topic_word = pd.merge(user_topic_word, t1, on=['用户id','user_topic_id'], how='left')\n",
    "    del user_topic_word['回答字数'], t1\n",
    "    user_topic_word = user_topic_word.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "    print(user_topic_word.info())\n",
    "    user_topic_word = reduce_mem_usage(user_topic_word)\n",
    "    \n",
    "    label_question = train_label['qid'].to_frame()\n",
    "    label_question = label_question.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "    question_topic = pd.merge(label_question, answer_qestion_info, left_on='qid', right_on='问题id', how='left').drop(['qid'],axis=1)\n",
    "    question_topic = question_info[['问题id','问题绑定话题']].drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "    #print(question_topic.info())\n",
    "    \n",
    "    question_topic1 = question_topic['问题绑定话题'].str.split(',', expand=True).stack()\n",
    "    question_topic1 = question_topic1.reset_index(level=1, drop=True).rename('question_topic_id')\n",
    "    question_topic = question_topic.join(question_topic1)\n",
    "    del question_topic1, question_topic['问题绑定话题']\n",
    "    #print(question_topic.head(10))\n",
    "    print(question_topic.info())\n",
    "    question_topic = reduce_mem_usage(question_topic)\n",
    "    \n",
    "    label_users = train_label['uid'].to_frame()\n",
    "    label_users = label_users.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "    user_topic_word = pd.merge(label_users, user_topic_word, left_on='uid', right_on='用户id', how='left').drop(['uid'], axis=1)\n",
    "    #topic_id encode一下\n",
    "    print(user_topic_word.info())\n",
    "    \n",
    "    user_topic_word['u_topic_word_count'] = user_topic_word['u_topic_word_count'].fillna(-1)\n",
    "    user_topic_word['u_topic_word_count'] = user_topic_word['u_topic_word_count'].astype('int')\n",
    "    user_topic_word = reduce_mem_usage(user_topic_word)\n",
    "    question_topic = question_topic.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "    user_topic_word = user_topic_word.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "    print(user_topic_word.info())\n",
    "    \n",
    "    with open(etype+'question_topic.pkl', 'wb') as file:\n",
    "        pickle.dump(question_topic, file)\n",
    "    with open(etype+'user_topic_word.pkl', 'wb') as file:\n",
    "        pickle.dump(user_topic_word, file)    \n",
    "    print(\"saved pkl ...\") \n",
    "    gc.collect()\n",
    "    \n",
    "    return 0\n",
    "\n",
    "# for users_topics\n",
    "def mk(ls_pair):\n",
    "    dt = {}\n",
    "    for pair in ls_pair:\n",
    "        dt[pair[0]] = pair[1]\n",
    "    return dt\n",
    "\n",
    "def mk_u_t_dict(ut_test):\n",
    "    print(\"start making u_topic_scores...\")\n",
    "    ut_test = ut_test.drop(index=[0], axis=0).reset_index() # 因为第一行全部都是0，所以删掉。。\n",
    "    df = ut_test[['user_topic_id','u_topic_word_count']]\n",
    "    ut_test['dict'] = pd.Series(df.to_dict('split')['data'])\n",
    "    del df, ut_test['index']\n",
    "    ut_test = ut_test.groupby('用户id')['dict'].agg(list).to_frame().reset_index()\n",
    "    ut_test['dict'] = ut_test['dict'].map(mk)\n",
    "    print(ut_test)\n",
    "\n",
    "    return ut_test\n",
    "\n",
    "# for qustion_topics\n",
    "def ab(df):\n",
    "    return ','.join(df.values)\n",
    "\n",
    "def mk_q_t(question_topic):\n",
    "    print(\"start making q_topics...\")\n",
    "    question_topic = question_topic.groupby(['问题id'])['question_topic_id'].apply(ab).to_frame()\n",
    "    question_topic = question_topic.reset_index()\n",
    "    print(question_topic)\n",
    "    \n",
    "    return question_topic\n",
    "\n",
    "\n",
    "# for the whole run\n",
    "def extract_user_topic_word_count(train_for_save, inv_label, etype):\n",
    "    q_t_file = etype+'question_topic.pkl'\n",
    "    u_t_w_file = etype+'user_topic_word.pkl'\n",
    "    if not (os.path.exists(q_t_file) and os.path.exists(u_t_w_file)):\n",
    "        creat_2pkl(answer_qestion_info, inv_label, etype)\n",
    "    \n",
    "    with open(q_t_file, 'rb') as file:\n",
    "        question_topic = pickle.load(file)\n",
    "    with open(u_t_w_file, 'rb') as file:\n",
    "        user_topic_word = pickle.load(file)\n",
    "    \n",
    "    user_topic_word = mk_u_t_dict(user_topic_word)\n",
    "    question_topic = mk_q_t(question_topic)    \n",
    "    \n",
    "    inv_label = pd.merge(inv_label, user_topic_word, left_on='uid', right_on='用户id', how='inner').drop(['用户id'], axis=1)\n",
    "    inv_label = pd.merge(inv_label, question_topic, left_on='qid', right_on='问题id', how='inner').drop(['问题id'], axis=1)\n",
    "    \n",
    "    print(inv_label.info())\n",
    "    print(inv_label.head(30))\n",
    "    return inv_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计数topic关联回答的个数-计算topic热度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_hot(answer_qestion_info, train_label, etype):\n",
    "    answer_qestion_info1 = answer_qestion_info['问题绑定话题'].str.split(',', expand=True).stack()\n",
    "    #print(answer_qestion_info1)\n",
    "    answer_qestion_info1 = answer_qestion_info1.reset_index(level=1, drop=True).rename('user_topic_id')\n",
    "    answer_qestion_info = answer_qestion_info.join(answer_qestion_info1)\n",
    "    #print(answer_qestion_info.head(10))\n",
    "    del answer_qestion_info1\n",
    "    answer_qestion_info = reduce_mem_usage(answer_qestion_info)\n",
    "    #print(answer_qestion_info.head(30))\n",
    "    \n",
    "    topic_list = answer_qestion_info[['问题id','user_topic_id']]\n",
    "    topic_list['topic_count'] = topic_list['user_topic_id'].map(topic_list['user_topic_id'].value_counts().astype(int))\n",
    "    topic_list = topic_list[['user_topic_id','topic_count']].drop_duplicates(inplace=False)\n",
    "    del answer_qestion_info\n",
    "    \n",
    "    print(topic_list.head(30))\n",
    "    with open(etype+'topic_count.pkl', 'wb') as file:\n",
    "        pickle.dump(topic_count, file)\n",
    "    return topic_list\n",
    "\n",
    "def merge_que_topic_count(topic_count, etype):\n",
    "    \n",
    "    with open(etype+'question_topic.pkl', 'rb') as file:\n",
    "        question_topic = pickle.load(file)\n",
    "    print(question_topic.head(5))\n",
    "    \n",
    "    question_label = pd.merge(question_topic, topic_count, left_on='question_topic_id', right_on='user_topic_id', how='inner').drop('question_topic_id',axis=1)\n",
    "    question_label['q_t_a_count_sum'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_count'].agg('sum').astype(int))\n",
    "    question_label['q_t_a_count_mean'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_count'].agg('mean'))\n",
    "    question_label['q_t_a_count_max'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_count'].agg('max').astype(int))\n",
    "    question_label['q_t_a_count_std'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_count'].agg('std'))\n",
    "    question_label['q_t_a_count_min'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_count'].agg('min').astype(int))\n",
    "    \n",
    "    question_label = question_label[['问题id','q_t_a_count_sum','q_t_a_count_mean','q_t_a_count_max','q_t_a_count_min','q_t_a_count_std']].drop_duplicates(inplace=False)\n",
    "    \n",
    "    with open(etype+'que_topic_count.pkl', 'wb') as file:\n",
    "        pickle.dump(question_label, file)\n",
    "    \n",
    "    return question_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 2593669"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiyangguang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start making u_topic_scores...\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_hdf(os.path.join(features_path,'data_all.h5'),key='data').reset_index()\n",
    "train_label = data.iloc[:train_len][['uid','qid']].drop_duplicates(inplace=False)\n",
    "test = data.iloc[train_len:][['uid','qid']].drop_duplicates(inplace=False)\n",
    "\n",
    "train_start = 3807\n",
    "train_end = 3860\n",
    "val_start = train_start + 7\n",
    "val_end = train_end + 7\n",
    "\n",
    "train_for_save2 = answer_qestion_info[answer_qestion_info['回答创建时间-day']>=train_start][answer_qestion_info['回答创建时间-day']<=train_end]\n",
    "test_for_save2 = answer_qestion_info[answer_qestion_info['回答创建时间-day']>=val_start][answer_qestion_info['回答创建时间-day']<=val_end]\n",
    "\n",
    "extract_user_topic_word_count(train_for_save2, train_label, 'train')\n",
    "extract_user_topic_word_count(test_for_save2, test, 'val')\n",
    "\n",
    "train_topic_count = get_topic_hot(train_for_save2, train_label, 'train')\n",
    "train_que_topic_count = merge_que_topic_count(train_topic_count, 'train')\n",
    "\n",
    "test_topic_count = get_topic_hot(test_for_save2, test, 'val')\n",
    "test_que_topic_count = merge_que_topic_count(test_topic_count, 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算topic的关联回答的点赞反馈-计算topic的质量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_good(answer_qestion_info, train_label, etype):\n",
    "    answer_qestion_info1 = answer_qestion_info['问题绑定话题'].str.split(',', expand=True).stack()\n",
    "    #print(answer_qestion_info1)\n",
    "    answer_qestion_info1 = answer_qestion_info1.reset_index(level=1, drop=True).rename('user_topic_id')\n",
    "    answer_qestion_info = answer_qestion_info.join(answer_qestion_info1)\n",
    "    #print(answer_qestion_info.head(10))\n",
    "    del answer_qestion_info1\n",
    "    answer_qestion_info = reduce_mem_usage(answer_qestion_info)\n",
    "    print(answer_qestion_info.head(30))\n",
    "    \n",
    "    topic_list = answer_qestion_info[['问题id','user_topic_id', '点赞数']]\n",
    "    topic_list['topic_good_count'] = topic_list['user_topic_id'].map(topic_list.groupby('user_topic_id')['点赞数'].agg('sum').astype(int))\n",
    "    topic_list = topic_list[['user_topic_id','topic_good_count']].drop_duplicates(inplace=False)\n",
    "    del answer_qestion_info  \n",
    "    \n",
    "    print(topic_list.head(30))\n",
    "    with open(etype+'topic_good_count.pkl', 'wb') as file:\n",
    "        pickle.dump(topic_count, file)\n",
    "    return topic_list\n",
    "\n",
    "def merge_que_topic_good(topic_count, etype):\n",
    "    \n",
    "    with open(etype+'question_topic.pkl', 'rb') as file:\n",
    "        question_topic = pickle.load(file)\n",
    "    print(question_topic.head(5))\n",
    "    \n",
    "    question_label = pd.merge(question_topic, topic_count, left_on='question_topic_id', right_on='user_topic_id', how='inner').drop('question_topic_id',axis=1)\n",
    "    question_label['q_t_a_good_count_sum'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_good_count'].agg('sum').astype(int))\n",
    "    question_label['q_t_a_good_count_mean'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_good_count'].agg('mean'))\n",
    "    question_label['q_t_a_good_count_max'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_good_count'].agg('max').astype(int))\n",
    "    question_label['q_t_a_good_count_std'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_good_count'].agg('std'))\n",
    "    question_label['q_t_a_good_count_min'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_good_count'].agg('min').astype(int))\n",
    "    \n",
    "    question_label = question_label[['问题id','q_t_a_good_count_sum','q_t_a_good_count_mean','q_t_a_good_count_max','q_t_a_good_count_min','q_t_a_good_count_std']].drop_duplicates(inplace=False)\n",
    "    \n",
    "    with open(etype+'que_topic_good_count.pkl', 'wb') as file:\n",
    "        pickle.dump(question_label, file)\n",
    "        \n",
    "    print(question_label.head())\n",
    "    \n",
    "    return question_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_hdf(os.path.join(features_path,'data_all.h5'),key='data').reset_index()\n",
    "train_label = data.iloc[:train_len][['uid','qid']].drop_duplicates(inplace=False)\n",
    "test = data.iloc[train_len:][['uid','qid']].drop_duplicates(inplace=False)\n",
    "\n",
    "train_start = 3807\n",
    "train_end = 3860\n",
    "val_start = train_start + 7\n",
    "val_end = train_end + 7\n",
    "\n",
    "train_for_save2 = answer_qestion_info[answer_qestion_info['回答创建时间-day']>=train_start][answer_qestion_info['回答创建时间-day']<=train_end]\n",
    "test_for_save2 = answer_qestion_info[answer_qestion_info['回答创建时间-day']>=val_start][answer_qestion_info['回答创建时间-day']<=val_end]\n",
    "\n",
    "train_topic_good_count = get_topic_good(train_for_save2, train_label, 'train')\n",
    "train_que_topic_good_count = merge_que_topic_good(train_topic_good_count, 'train')\n",
    "\n",
    "test_topic_good_count = get_topic_good(test_for_save2, test, 'val')\n",
    "test_que_topic_good_count = merge_que_topic_good(test_topic_good_count, 'val')\n",
    "\n",
    "print(train_que_topic_good_count.info())\n",
    "print(test_que_topic_good_count.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据topic关联回答的收藏反馈- 计算topic的质量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_mark(answer_qestion_info, train_label, etype):\n",
    "    answer_qestion_info1 = answer_qestion_info['问题绑定话题'].str.split(',', expand=True).stack()\n",
    "    #print(answer_qestion_info1)\n",
    "    answer_qestion_info1 = answer_qestion_info1.reset_index(level=1, drop=True).rename('user_topic_id')\n",
    "    answer_qestion_info = answer_qestion_info.join(answer_qestion_info1)\n",
    "    #print(answer_qestion_info.head(10))\n",
    "    del answer_qestion_info1\n",
    "    answer_qestion_info = reduce_mem_usage(answer_qestion_info)\n",
    "    print(answer_qestion_info.head(30))\n",
    "    \n",
    "    topic_list = answer_qestion_info[['问题id','user_topic_id', '收藏数']]\n",
    "    topic_list['topic_mark_count'] = topic_list['user_topic_id'].map(topic_list.groupby('user_topic_id')['收藏数'].agg('sum').astype(int))\n",
    "    topic_list = topic_list[['user_topic_id','topic_mark_count']].drop_duplicates(inplace=False)\n",
    "    del answer_qestion_info  \n",
    "    \n",
    "    print(topic_list.head(30))\n",
    "    with open(etype+'topic_mark_count.pkl', 'wb') as file:\n",
    "        pickle.dump(topic_count, file)\n",
    "    return topic_list\n",
    "\n",
    "def merge_que_topic_mark(topic_count, etype):\n",
    "    \n",
    "    with open(etype+'question_topic.pkl', 'rb') as file:\n",
    "        question_topic = pickle.load(file)\n",
    "    print(question_topic.head(5))\n",
    "    \n",
    "    question_label = pd.merge(question_topic, topic_count, left_on='question_topic_id', right_on='user_topic_id', how='inner').drop('question_topic_id',axis=1)\n",
    "    question_label['q_t_a_mark_count_sum'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_mark_count'].agg('sum').astype(int))\n",
    "    question_label['q_t_a_mark_count_mean'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_mark_count'].agg('mean'))\n",
    "    question_label['q_t_a_mark_count_max'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_mark_count'].agg('max').astype(int))\n",
    "    question_label['q_t_a_mark_count_std'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_mark_count'].agg('std'))\n",
    "    question_label['q_t_a_mark_count_min'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_mark_count'].agg('min').astype(int))\n",
    "    \n",
    "    question_label = question_label[['问题id','q_t_a_mark_count_sum','q_t_a_mark_count_mean','q_t_a_mark_count_max','q_t_a_mark_count_min','q_t_a_mark_count_std']].drop_duplicates(inplace=False)\n",
    "    \n",
    "    with open(etype+'que_topic_mark_count.pkl', 'wb') as file:\n",
    "        pickle.dump(question_label, file)\n",
    "        \n",
    "    print(question_label.head())\n",
    "    \n",
    "    return question_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_hdf(os.path.join(features_path,'data_all.h5'),key='data').reset_index()\n",
    "train_label = data.iloc[:train_len][['uid','qid']].drop_duplicates(inplace=False)\n",
    "test = data.iloc[train_len:][['uid','qid']].drop_duplicates(inplace=False)\n",
    "\n",
    "train_start = 3807\n",
    "train_end = 3860\n",
    "val_start = train_start + 7\n",
    "val_end = train_end + 7\n",
    "\n",
    "train_for_save2 = answer_qestion_info[answer_qestion_info['回答创建时间-day']>=train_start][answer_qestion_info['回答创建时间-day']<=train_end]\n",
    "test_for_save2 = answer_qestion_info[answer_qestion_info['回答创建时间-day']>=val_start][answer_qestion_info['回答创建时间-day']<=val_end]\n",
    "\n",
    "train_topic_mark_count = get_topic_mark(train_for_save2, train_label, 'train')\n",
    "train_que_topic_mark_count = merge_que_topic_mark(train_topic_mark_count, 'train')\n",
    "\n",
    "test_topic_mark_count = get_topic_mark(test_for_save2, test, 'val')\n",
    "test_que_topic_mark_count = merge_que_topic_mark(test_topic_mark_count, 'val')\n",
    "\n",
    "print(train_que_topic_mark_count.info())\n",
    "print(test_que_topic_mark_count.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据topic关联回答的感谢反馈- 计算topic的质量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_thank(answer_qestion_info, train_label, etype):\n",
    "    answer_qestion_info1 = answer_qestion_info['问题绑定话题'].str.split(',', expand=True).stack()\n",
    "    #print(answer_qestion_info1)\n",
    "    answer_qestion_info1 = answer_qestion_info1.reset_index(level=1, drop=True).rename('user_topic_id')\n",
    "    answer_qestion_info = answer_qestion_info.join(answer_qestion_info1)\n",
    "    #print(answer_qestion_info.head(10))\n",
    "    del answer_qestion_info1\n",
    "    answer_qestion_info = reduce_mem_usage(answer_qestion_info)\n",
    "    print(answer_qestion_info.head(30))\n",
    "    \n",
    "    topic_list = answer_qestion_info[['问题id','user_topic_id', '感谢数']]\n",
    "    topic_list['topic_thank_count'] = topic_list['user_topic_id'].map(topic_list.groupby('user_topic_id')['感谢数'].agg('sum').astype(int))\n",
    "    topic_list = topic_list[['user_topic_id','topic_thank_count']].drop_duplicates(inplace=False)\n",
    "    del answer_qestion_info  \n",
    "    \n",
    "    print(topic_list.head(30))\n",
    "    with open(etype+'topic_thank_count.pkl', 'wb') as file:\n",
    "        pickle.dump(topic_count, file)\n",
    "    return topic_list\n",
    "\n",
    "def merge_que_topic_thank(topic_count, etype):\n",
    "    \n",
    "    with open(etype+'question_topic.pkl', 'rb') as file:\n",
    "        question_topic = pickle.load(file)\n",
    "    print(question_topic.head(5))\n",
    "    \n",
    "    question_label = pd.merge(question_topic, topic_count, left_on='question_topic_id', right_on='user_topic_id', how='inner').drop('question_topic_id',axis=1)\n",
    "    question_label['q_t_a_thank_count_sum'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_thank_count'].agg('sum').astype(int))\n",
    "    question_label['q_t_a_thank_count_mean'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_thank_count'].agg('mean'))\n",
    "    question_label['q_t_a_thank_count_max'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_thank_count'].agg('max').astype(int))\n",
    "    question_label['q_t_a_thank_count_std'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_thank_count'].agg('std'))\n",
    "    question_label['q_t_a_thank_count_min'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_thank_count'].agg('min').astype(int))\n",
    "    \n",
    "    question_label = question_label[['问题id','q_t_a_thank_count_sum','q_t_a_thank_count_mean','q_t_a_thank_count_max','q_t_a_thank_count_min','q_t_a_thank_count_std']].drop_duplicates(inplace=False)\n",
    "    \n",
    "    with open(etype+'que_topic_thank_count.pkl', 'wb') as file:\n",
    "        pickle.dump(question_label, file)\n",
    "        \n",
    "    print(question_label.head())\n",
    "    \n",
    "    return question_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_hdf(os.path.join(features_path,'data_all.h5'),key='data').reset_index()\n",
    "train_label = data.iloc[:train_len][['uid','qid']].drop_duplicates(inplace=False)\n",
    "test = data.iloc[train_len:][['uid','qid']].drop_duplicates(inplace=False)\n",
    "\n",
    "train_start = 3807\n",
    "train_end = 3860\n",
    "val_start = train_start + 7\n",
    "val_end = train_end + 7\n",
    "\n",
    "train_for_save2 = answer_qestion_info[answer_qestion_info['回答创建时间-day']>=train_start][answer_qestion_info['回答创建时间-day']<=train_end]\n",
    "test_for_save2 = answer_qestion_info[answer_qestion_info['回答创建时间-day']>=val_start][answer_qestion_info['回答创建时间-day']<=val_end]\n",
    "\n",
    "train_topic_thank_count = get_topic_thank(train_for_save2, train_label, 'train')\n",
    "train_que_topic_thank_count = merge_que_topic_thank(train_topic_thank_count, 'train')\n",
    "\n",
    "test_topic_thank_count = get_topic_thank(test_for_save2, test, 'val')\n",
    "test_que_topic_thank_count = merge_que_topic_thank(test_topic_thank_count, 'val')\n",
    "\n",
    "print(train_que_topic_thank_count.info())\n",
    "print(test_que_topic_thank_count.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算topic在inv中的接受程度 - 计算topic的转化率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_trans(inv_qestion_info, train_label, etype):\n",
    "    inv_qestion_info1 = inv_qestion_info['问题绑定话题'].str.split(',', expand=True).stack()\n",
    "    #print(inv_qestion_info1)\n",
    "    inv_qestion_info1 = inv_qestion_info1.reset_index(level=1, drop=True).rename('user_inv_topic_id')\n",
    "    inv_qestion_info = inv_qestion_info.join(inv_qestion_info1)\n",
    "    #print(inv_qestion_info.head(10))\n",
    "    del inv_qestion_info1\n",
    "    inv_qestion_info = reduce_mem_usage(inv_qestion_info)\n",
    "    print(inv_qestion_info.head(30))\n",
    "    \n",
    "    topic_list = inv_qestion_info[['问题id','user_inv_topic_id', '是否回答']]\n",
    "    topic_list['topic_inv_pos_mean'] = topic_list['user_inv_topic_id'].map(topic_list.groupby('user_inv_topic_id')['是否回答'].agg('mean'))\n",
    "    topic_list['topic_inv_neg_mean'] = topic_list['user_inv_topic_id'].map(topic_list.groupby('user_inv_topic_id')['是否回答'].agg(lambda x: 1-np.mean(x)))\n",
    "    topic_list['topic_inv_sum'] = topic_list['user_inv_topic_id'].map(topic_list.groupby('user_inv_topic_id')['是否回答'].agg('sum').astype(int))\n",
    "    topic_list['topic_inv_std'] = topic_list['user_inv_topic_id'].map(topic_list.groupby('user_inv_topic_id')['是否回答'].agg('std'))\n",
    "    topic_list['topic_inv_count'] = topic_list['user_inv_topic_id'].map(topic_list['user_inv_topic_id'].value_counts().astype(int))\n",
    "\n",
    "    topic_list = topic_list[['user_inv_topic_id','topic_inv_pos_mean','topic_inv_neg_mean','topic_inv_sum','topic_inv_std','topic_inv_count']].drop_duplicates(inplace=False)\n",
    "    del inv_qestion_info  \n",
    "    \n",
    "    print(topic_list.head(30))\n",
    "    with open(etype+'topic_inv_count.pkl', 'wb') as file:\n",
    "        pickle.dump(topic_count, file)\n",
    "    return topic_list\n",
    "\n",
    "def merge_que_topic_trans(topic_count, etype):\n",
    "    \n",
    "    with open(etype+'question_topic.pkl', 'rb') as file:\n",
    "        question_topic = pickle.load(file)\n",
    "    print(question_topic.head(5))\n",
    "    \n",
    "    question_label = pd.merge(question_topic, topic_count, left_on='question_topic_id', right_on='user_inv_topic_id', how='inner').drop('question_topic_id',axis=1)\n",
    "    #question_label = topic_count\n",
    "    question_label['q_t_a_inv_mean_sum'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_inv_pos_mean'].agg('sum').astype(int))\n",
    "    question_label['q_t_a_inv_mean_mean'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_inv_pos_mean'].agg('mean'))\n",
    "    question_label['q_t_a_inv_mean_max'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_inv_pos_mean'].agg('max').astype(int))\n",
    "    question_label['q_t_a_inv_mean_std'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_inv_pos_mean'].agg('std'))\n",
    "    question_label['q_t_a_inv_mean_min'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_inv_pos_mean'].agg('min').astype(int))\n",
    "    \n",
    "    question_label['q_t_a_inv_std_mean'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_inv_std'].agg('mean'))\n",
    "    question_label['q_t_a_inv_sum_mean'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_inv_sum'].agg('mean'))\n",
    "    question_label['q_t_a_inv_count_mean'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_inv_count'].agg('mean'))\n",
    "\n",
    "    question_label['q_t_a_neg_inv_mean_sum'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_inv_neg_mean'].agg('sum').astype(int))\n",
    "    question_label['q_t_a_neg_inv_mean_mean'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_inv_neg_mean'].agg('mean'))\n",
    "    question_label['q_t_a_neg_inv_mean_max'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_inv_neg_mean'].agg('max').astype(int))\n",
    "    question_label['q_t_a_neg_inv_mean_std'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_inv_neg_mean'].agg('std'))\n",
    "    question_label['q_t_a_neg_inv_mean_min'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_inv_neg_mean'].agg('min').astype(int))\n",
    "    \n",
    "  \n",
    "    \n",
    "    question_label = question_label[['问题id','q_t_a_inv_mean_sum','q_t_a_inv_mean_mean','q_t_a_inv_mean_max','q_t_a_inv_mean_min','q_t_a_inv_mean_std',\n",
    "                                    'q_t_a_inv_std_mean', 'q_t_a_inv_sum_mean', 'q_t_a_inv_count_mean', \n",
    "                                     'q_t_a_neg_inv_mean_sum', 'q_t_a_neg_inv_mean_mean', 'q_t_a_neg_inv_mean_max', 'q_t_a_neg_inv_mean_std', 'q_t_a_neg_inv_mean_min'\n",
    "                                    ]].drop_duplicates(inplace=False)\n",
    "    \n",
    "    with open(etype+'que_topic_inv_count.pkl', 'wb') as file:\n",
    "        pickle.dump(question_label, file)\n",
    "        \n",
    "    print(question_label.head())\n",
    "    \n",
    "    return question_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_label.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_qestion_info = pd.read_csv(os.path.join(data_path, 'invite_info_0926.txt'), header=None, sep='\\t')\n",
    "inv_qestion_info.columns = ['问题id', '用户id', '邀请创建时间','是否回答']\n",
    "inv_qestion_info['邀请创建时间-day'] = inv_qestion_info['邀请创建时间'].apply(lambda x:x.split('-')[0].split('D')[1]).astype(int)\n",
    "del inv_qestion_info['邀请创建时间'] \n",
    "\n",
    "data = pd.read_hdf(os.path.join(features_path,'data_all.h5'),key='data').reset_index()\n",
    "train_label = data.iloc[:train_len][['uid','qid','label','day']].drop_duplicates(inplace=False)\n",
    "train_label = pd.merge(train_label, question_info, left_on='qid', right_on='问题id', how='left').drop(['问题id'], axis=1, inplace=False)\n",
    "\n",
    "test = data.iloc[train_len:][['uid','qid','label','day']].drop_duplicates(inplace=False)\n",
    "test = pd.merge(test, question_info, left_on='qid', right_on='问题id', how='left').drop(['问题id'], axis=1, inplace=False)\n",
    "\n",
    "train_start = 3837\n",
    "train_end = 3860\n",
    "val_start = train_start + 7\n",
    "val_end = train_end + 7\n",
    "\n",
    "train_for_save3 = inv_qestion_info[inv_qestion_info['邀请创建时间-day']>=train_start][inv_qestion_info['邀请创建时间-day']<=train_end]\n",
    "test_for_save3 = inv_qestion_info[inv_qestion_info['邀请创建时间-day']>=val_start][inv_qestion_info['邀请创建时间-day']<=val_end]\n",
    "train_for_save3 = pd.merge(train_for_save3, question_info, on='问题id', how='left')\n",
    "test_for_save3 = pd.merge(test_for_save3, question_info, on='问题id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_for_save3.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_topic_trans = get_topic_trans(train_for_save3, train_label, 'train')\n",
    "train_que_topic_trans = merge_que_topic_trans(train_topic_trans, 'train')\n",
    "\n",
    "test_topic_trans = get_topic_trans(test_for_save3, test, 'val')\n",
    "test_que_topic_trans = merge_que_topic_trans(test_topic_trans, 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结汇总到一个表里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q_a_topic = pd.merge(train_que_topic_count, train_que_topic_good_count, on='问题id', how='outer')\n",
    "test_q_a_topic = pd.merge(test_que_topic_count, test_que_topic_good_count, on='问题id', how='outer')\n",
    "train_q_a_topic = pd.merge(train_q_a_topic, train_que_topic_trans, on='问题id', how='outer')\n",
    "test_q_a_topic = pd.merge(test_q_a_topic, test_que_topic_trans, on='问题id', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1828312 entries, 0 to 1828311\n",
      "Data columns (total 34 columns):\n",
      "问题id                       object\n",
      "q_t_a_count_sum            float64\n",
      "q_t_a_count_mean           float64\n",
      "q_t_a_count_max            float64\n",
      "q_t_a_count_min            float64\n",
      "q_t_a_count_std            float64\n",
      "q_t_a_good_count_sum       float64\n",
      "q_t_a_good_count_mean      float64\n",
      "q_t_a_good_count_max       float64\n",
      "q_t_a_good_count_min       float64\n",
      "q_t_a_good_count_std       float64\n",
      "q_t_a_inv_mean_sum         float64\n",
      "q_t_a_inv_mean_mean        float64\n",
      "q_t_a_inv_mean_max         float64\n",
      "q_t_a_inv_mean_min         float64\n",
      "q_t_a_inv_mean_std         float64\n",
      "q_t_a_inv_std_mean         float64\n",
      "q_t_a_inv_sum_mean         float64\n",
      "q_t_a_inv_count_mean       float64\n",
      "q_t_a_neg_inv_mean_sum     float64\n",
      "q_t_a_neg_inv_mean_mean    float64\n",
      "q_t_a_neg_inv_mean_max     float64\n",
      "q_t_a_neg_inv_mean_std     float64\n",
      "q_t_a_neg_inv_mean_min     float64\n",
      "q_t_a_mark_count_sum       float64\n",
      "q_t_a_mark_count_mean      float64\n",
      "q_t_a_mark_count_max       float64\n",
      "q_t_a_mark_count_min       float64\n",
      "q_t_a_mark_count_std       float64\n",
      "q_t_a_thank_count_sum      float64\n",
      "q_t_a_thank_count_mean     float64\n",
      "q_t_a_thank_count_max      float64\n",
      "q_t_a_thank_count_min      float64\n",
      "q_t_a_thank_count_std      float64\n",
      "dtypes: float64(33), object(1)\n",
      "memory usage: 488.2+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1829266 entries, 0 to 1829265\n",
      "Data columns (total 34 columns):\n",
      "问题id                       object\n",
      "q_t_a_count_sum            float64\n",
      "q_t_a_count_mean           float64\n",
      "q_t_a_count_max            float64\n",
      "q_t_a_count_min            float64\n",
      "q_t_a_count_std            float64\n",
      "q_t_a_good_count_sum       float64\n",
      "q_t_a_good_count_mean      float64\n",
      "q_t_a_good_count_max       float64\n",
      "q_t_a_good_count_min       float64\n",
      "q_t_a_good_count_std       float64\n",
      "q_t_a_inv_mean_sum         float64\n",
      "q_t_a_inv_mean_mean        float64\n",
      "q_t_a_inv_mean_max         float64\n",
      "q_t_a_inv_mean_min         float64\n",
      "q_t_a_inv_mean_std         float64\n",
      "q_t_a_inv_std_mean         float64\n",
      "q_t_a_inv_sum_mean         float64\n",
      "q_t_a_inv_count_mean       float64\n",
      "q_t_a_neg_inv_mean_sum     float64\n",
      "q_t_a_neg_inv_mean_mean    float64\n",
      "q_t_a_neg_inv_mean_max     float64\n",
      "q_t_a_neg_inv_mean_std     float64\n",
      "q_t_a_neg_inv_mean_min     float64\n",
      "q_t_a_mark_count_sum       float64\n",
      "q_t_a_mark_count_mean      float64\n",
      "q_t_a_mark_count_max       float64\n",
      "q_t_a_mark_count_min       float64\n",
      "q_t_a_mark_count_std       float64\n",
      "q_t_a_thank_count_sum      float64\n",
      "q_t_a_thank_count_mean     float64\n",
      "q_t_a_thank_count_max      float64\n",
      "q_t_a_thank_count_min      float64\n",
      "q_t_a_thank_count_std      float64\n",
      "dtypes: float64(33), object(1)\n",
      "memory usage: 488.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train_q_a_topic = pd.merge(train_q_a_topic, train_que_topic_mark_count, on='问题id', how='outer')\n",
    "test_q_a_topic = pd.merge(test_q_a_topic, train_que_topic_mark_count, on='问题id', how='outer')\n",
    "\n",
    "train_q_a_topic = pd.merge(train_q_a_topic, train_que_topic_thank_count, on='问题id', how='outer')\n",
    "test_q_a_topic = pd.merge(test_q_a_topic, train_que_topic_thank_count, on='问题id', how='outer')\n",
    "\n",
    "print(train_q_a_topic.info())\n",
    "print(test_q_a_topic.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('que_topic_all_count_train.pkl', 'wb') as file:\n",
    "    pickle.dump(train_q_a_topic, file)\n",
    "with open('que_topic_all_count_val.pkl', 'wb') as file:\n",
    "    pickle.dump(test_q_a_topic, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------------分割线------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiyangguang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_hdf(os.path.join(features_path,'data_all.h5'),key='data').reset_index()\n",
    "train_label = data.iloc[:train_len][['uid','qid']].drop_duplicates(inplace=False)\n",
    "test = data.iloc[train_len:][['uid','qid']].drop_duplicates(inplace=False)\n",
    "\n",
    "train_start = 3807\n",
    "train_end = 3860\n",
    "val_start = train_start + 7\n",
    "val_end = train_end + 7\n",
    "\n",
    "train_for_save = answer_qestion_info[answer_qestion_info['回答创建时间-day']>=train_start][answer_qestion_info['回答创建时间-day']<=train_end]\n",
    "test_for_save = answer_qestion_info[answer_qestion_info['回答创建时间-day']>=val_start][answer_qestion_info['回答创建时间-day']<=val_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3828707 entries, 0 to 4513734\n",
      "Data columns (total 9 columns):\n",
      "回答id          object\n",
      "用户id          object\n",
      "问题id          object\n",
      "问题绑定话题        object\n",
      "回答字数          int64\n",
      "点赞数           int64\n",
      "感谢数           int64\n",
      "收藏数           int64\n",
      "回答创建时间-day    int64\n",
      "dtypes: int64(5), object(4)\n",
      "memory usage: 292.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train_for_save.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2593066 entries, 0 to 2593668\n",
      "Data columns (total 2 columns):\n",
      "uid    object\n",
      "qid    object\n",
      "dtypes: object(2)\n",
      "memory usage: 59.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train_label.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start making u_topic_scores...\n",
      "               用户id                                               dict\n",
      "0       M1000000382                              {'T13': 0, 'T567': 0}\n",
      "1       M1000000983  {'T6192': 8, 'T1667': 56, 'T2255': 31, 'T1103'...\n",
      "2       M1000008978  {'T1891': 6, 'T56': 6, 'T5': 6, 'T39': 6, 'T13...\n",
      "3       M1000020034                                       {'T597': 90}\n",
      "4       M1000025214                           {'T86': 226, 'T74': 226}\n",
      "5       M1000031027  {'T49': 24, 'T82': 24, 'T72': 30, 'T42': 30, '...\n",
      "6       M1000043012  {'T4557': 710, 'T543': 506, 'T1655': 277, 'T55...\n",
      "7       M1000047678  {'T7': 10, 'T2': 10, 'T35': 10, 'T252': 10, 'T...\n",
      "8       M1000052881  {'T1201': 95, 'T10855': 95, 'T11899': 95, 'T41...\n",
      "9       M1000054663  {'T539': 858, 'T132': 180, 'T272': 439, 'T491'...\n",
      "10      M1000055649                           {'T402': 56, 'T849': 56}\n",
      "11      M1000058328        {'T234': 7, 'T101': 7, 'T2': 7, 'T5807': 7}\n",
      "12      M1000063561  {'T37': 920, 'T79364': 29, 'T112': 29, 'T204':...\n",
      "13      M1000065584  {'T4410': 812, 'T1655': 694, 'T543': 873, 'T3'...\n",
      "14      M1000071070  {'T664': 13, 'T12751': 13, 'T660': 13, 'T93': ...\n",
      "15      M1000076506  {'T11906': 58, 'T81': 58, 'T6057': 27, 'T29193...\n",
      "16      M1000084325  {'T9825': 55, 'T5448': 55, 'T19624': 55, 'T819...\n",
      "17      M1000088648  {'T11': 29, 'T85': 29, 'T180': 29, 'T365': 29,...\n",
      "18      M1000094299  {'T42': 23, 'T78': 23, 'T255': 26, 'T131': 23,...\n",
      "19      M1000114377  {'T2141': 1553, 'T4341': 1553, 'T1005': 36, 'T...\n",
      "20      M1000141697  {'T24': 235, 'T791': 235, 'T417': 9, 'T1346': ...\n",
      "21       M100015013  {'T513': 4, 'T2783': 4, 'T1278': 4, 'T1397': 4...\n",
      "22      M1000158078              {'T48': 213, 'T121': 213, 'T13': 213}\n",
      "23       M100015868  {'T2529': 12, 'T1905': 12, 'T2977': 17, 'T9': ...\n",
      "24      M1000167043  {'T3873': 54, 'T1788': 54, 'T910': 54, 'T52': ...\n",
      "25      M1000174601  {'T181': 49, 'T269': 33, 'T729': 16, 'T5401': 16}\n",
      "26      M1000192704  {'T9902': 495, 'T388': 495, 'T595': 495, 'T640...\n",
      "27      M1000193118  {'T4873': 10, 'T1966': 24, 'T917': 10, 'T8161'...\n",
      "28      M1000212511  {'T1065': 26, 'T183': 31, 'T112': 31, 'T534': ...\n",
      "29      M1000217946  {'T650': 103, 'T1014': 103, 'T112': 283, 'T63'...\n",
      "...             ...                                                ...\n",
      "618757   M999788674  {'T918': 16, 'T29': 55, 'T354': 16, 'T4825': 1...\n",
      "618758   M999792591  {'T3634': 85, 'T7371': 107, 'T370': 61, 'T312'...\n",
      "618759   M999794397  {'T5673': 416, 'T1680': 416, 'T5871': 416, 'T5...\n",
      "618760   M999796050  {'T476': 44, 'T12': 44, 'T13': 99, 'T451': 30,...\n",
      "618761   M999799401                 {'T66': 48, 'T186': 48, 'T84': 48}\n",
      "618762   M999807176  {'T1039': 31, 'T17137': 62, 'T2859': 128, 'T22...\n",
      "618763   M999813324  {'T407': 61, 'T70': 1079, 'T513': 61, 'T100': ...\n",
      "618764   M999822793  {'T27011': 10, 'T20589': 10, 'T19794': 10, 'T2...\n",
      "618765   M999824350  {'T481': 533, 'T2898': 533, 'T45': 533, 'T1003...\n",
      "618766   M999827559                  {'T2': 74, 'T22': 74, 'T488': 74}\n",
      "618767   M999831339  {'T6948': 15, 'T52': 871, 'T15': 137, 'T3': 73...\n",
      "618768   M999845013                             {'T2': 63, 'T145': 63}\n",
      "618769   M999856979  {'T3651': 558, 'T105': 558, 'T2373': 558, 'T32...\n",
      "618770   M999866056  {'T4': 423, 'T54': 423, 'T24': 423, 'T8': 8081...\n",
      "618771   M999866124  {'T141': 153, 'T19': 215, 'T4006': 21, 'T5550'...\n",
      "618772   M999868450  {'T2911': 57, 'T234': 104, 'T93': 57, 'T6270':...\n",
      "618773   M999874749                             {'T17': 28, 'T27': 28}\n",
      "618774   M999895238  {'T1080': 20, 'T6144': 20, 'T1142': 20, 'T619'...\n",
      "618775   M999898000                           {'T13': 43, 'T6055': 43}\n",
      "618776    M99991380  {'T4946': 23, 'T20614': 23, 'T3932': 23, 'T762...\n",
      "618777   M999931159  {'T10': 137, 'T24': 137, 'T2215': 2066, 'T241'...\n",
      "618778   M999944901  {'T118': 6, 'T13': 6, 'T604': 6, 'T8808': 11, ...\n",
      "618779   M999946975  {'T49912': 15, 'T573': 15, 'T374': 1004, 'T212...\n",
      "618780   M999960131     {'T2506': 61, 'T7270': 2, 'T24': 59, 'T3': 59}\n",
      "618781   M999972516           {'T20610': 70, 'T2813': 70, 'T2549': 70}\n",
      "618782   M999979471                {'T7929': 0, 'T924': 0, 'T2899': 0}\n",
      "618783   M999984680  {'T26': 19, 'T277': 19, 'T76': 19, 'T245': 189...\n",
      "618784   M999995457  {'T2': 348, 'T488': 343, 'T11088': 42, 'T6248'...\n",
      "618785   M999998695                        {'T810': 206, 'T5568': 206}\n",
      "618786   M999998888  {'T35': 12, 'T296': 27, 'T4979': 12, 'T3': 15,...\n",
      "\n",
      "[618787 rows x 2 columns]\n",
      "start making q_topics...\n",
      "                问题id                question_topic_id\n",
      "0        Q1000000315        T3290,T495,T5471,T668,T79\n",
      "1        Q1000002524                            T1909\n",
      "2        Q1000003894                  T616,T227,T3433\n",
      "3        Q1000004883                    T112,T463,T10\n",
      "4        Q1000006560      T8302,T181,T7342,T1169,T259\n",
      "5        Q1000007604               T32200,T8545,T7305\n",
      "6        Q1000008276              T1615,T24,T456,T741\n",
      "7        Q1000010200            T46243,T24,T42,T20351\n",
      "8        Q1000020925                       T9128,T718\n",
      "9        Q1000021185   T3226,T600,T11240,T2361,T12104\n",
      "10       Q1000025766                        T38,T2187\n",
      "11        Q100002777                            T1217\n",
      "12       Q1000030162               T10079,T38,T762,T2\n",
      "13        Q100003566        T18296,T14059,T30048,T184\n",
      "14       Q1000036156                       T905,T4345\n",
      "15       Q1000037403                  T39,T1850,T2091\n",
      "16       Q1000037824                           T12572\n",
      "17       Q1000041691                     T3680,T11711\n",
      "18       Q1000049860             T97,T791,T456,T24091\n",
      "19       Q1000050745                             T893\n",
      "20        Q100005202                       T156,T1249\n",
      "21       Q1000055583                           T18880\n",
      "22       Q1000060170                       T157,T5166\n",
      "23       Q1000060407                        T66,T5810\n",
      "24       Q1000067956             T4894,T263,T561,T558\n",
      "25       Q1000069029                    T20,T15,T1239\n",
      "26       Q1000070819                             T310\n",
      "27       Q1000074692           T9804,T4872,T3098,T983\n",
      "28       Q1000075068                 T1142,T3363,T619\n",
      "29       Q1000077463               T3637,T1083,T17531\n",
      "...              ...                              ...\n",
      "1829870   Q999952782                       T95,T14674\n",
      "1829871    Q99995466         T24,T534,T456,T241,T2215\n",
      "1829872   Q999955973               T670,T240,T109,T26\n",
      "1829873   Q999958198                   T192,T663,T387\n",
      "1829874   Q999960053              T9,T265,T207,T16273\n",
      "1829875   Q999962141                T11058,T590,T6683\n",
      "1829876   Q999962730     T3697,T502,T11,T17382,T16863\n",
      "1829877   Q999967056                T184,T2710,T12843\n",
      "1829878    Q99996827                  T101,T5807,T234\n",
      "1829879   Q999969820                             T827\n",
      "1829880   Q999970938                         T166,T55\n",
      "1829881   Q999972709                  T361,T1122,T255\n",
      "1829882   Q999974398             T71,T371,T196,T27,T7\n",
      "1829883   Q999976398       T1370,T24,T3923,T378,T3838\n",
      "1829884   Q999977222                     T34763,T3218\n",
      "1829885   Q999977345                           T47175\n",
      "1829886   Q999978668     T768,T7004,T5421,T211,T42858\n",
      "1829887   Q999985030                           T13005\n",
      "1829888   Q999985860            T4532,T101,T664,T1293\n",
      "1829889   Q999986062         T970,T132,T272,T1675,T23\n",
      "1829890   Q999986135                         T693,T37\n",
      "1829891   Q999990752                 T28158,T250,T253\n",
      "1829892   Q999991269                T1776,T1044,T9516\n",
      "1829893   Q999992666       T2286,T2751,T839,T62,T2339\n",
      "1829894   Q999993263                       T1054,T463\n",
      "1829895   Q999993281               T7392,T16689,T9030\n",
      "1829896   Q999995156     T34334,T3247,T563,T34632,T27\n",
      "1829897   Q999996608               T11837,T49663,T417\n",
      "1829898   Q999997494  T1519,T12948,T14367,T1283,T2209\n",
      "1829899   Q999999307                   T281,T56,T4879\n",
      "\n",
      "[1829900 rows x 2 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2073213 entries, 0 to 2073212\n",
      "Data columns (total 4 columns):\n",
      "uid                  object\n",
      "qid                  object\n",
      "dict                 object\n",
      "question_topic_id    object\n",
      "dtypes: object(4)\n",
      "memory usage: 79.1+ MB\n",
      "None\n",
      "            uid          qid  \\\n",
      "0   M2317670257   Q604029601   \n",
      "1    M893603481   Q604029601   \n",
      "2   M1699039805   Q604029601   \n",
      "3   M3147978248   Q604029601   \n",
      "4    M866789582   Q604029601   \n",
      "5   M2317670257  Q2206379703   \n",
      "6   M3364034715  Q2206379703   \n",
      "7    M798635034  Q2206379703   \n",
      "8   M3572926493  Q2206379703   \n",
      "9   M1152434806  Q2206379703   \n",
      "10  M2786206127  Q2206379703   \n",
      "11  M2667175447  Q2206379703   \n",
      "12  M1982580265  Q2206379703   \n",
      "13   M209923168  Q2206379703   \n",
      "14   M995852737  Q2206379703   \n",
      "15  M3772681272  Q2206379703   \n",
      "16  M2818659842   Q795459266   \n",
      "17  M3457286118   Q795459266   \n",
      "18  M3494006438   Q795459266   \n",
      "19  M1159289094   Q795459266   \n",
      "20  M2818659842  Q3295690941   \n",
      "21  M1802096168  Q3295690941   \n",
      "22  M2634588371  Q3295690941   \n",
      "23  M1873828039  Q3295690941   \n",
      "24  M3164678143  Q3295690941   \n",
      "25  M2851727137  Q3295690941   \n",
      "26  M1801148355  Q3295690941   \n",
      "27  M2907987124  Q3295690941   \n",
      "28  M3362115931  Q3295690941   \n",
      "29  M1051962023  Q3295690941   \n",
      "\n",
      "                                                 dict  \\\n",
      "0   {'T10': 135, 'T10186': 135, 'T42650': 64, 'T18...   \n",
      "1   {'T4385': 44, 'T2863': 3956, 'T70': 4744, 'T91...   \n",
      "2   {'T2': 268, 'T47': 268, 'T22': 268, 'T3': 16, ...   \n",
      "3   {'T208': 335, 'T5339': 335, 'T927': 335, 'T271...   \n",
      "4   {'T32': 276, 'T81': 276, 'T524': 276, 'T2522':...   \n",
      "5   {'T10': 135, 'T10186': 135, 'T42650': 64, 'T18...   \n",
      "6   {'T5122': 835, 'T3299': 55, 'T1203': 2035, 'T4...   \n",
      "7          {'T2733': 177, 'T5122': 177, 'T1203': 177}   \n",
      "8   {'T2733': 69, 'T5122': 54, 'T1203': 69, 'T310'...   \n",
      "9   {'T271': 12, 'T517': 146, 'T3692': 146, 'T11':...   \n",
      "10  {'T3463': 506, 'T2733': 318, 'T7726': 289, 'T2...   \n",
      "11  {'T377': 199, 'T30': 199, 'T2685': 199, 'T7944...   \n",
      "12  {'T873': 64, 'T1203': 269, 'T3129': 64, 'T2733...   \n",
      "13  {'T4454': 730, 'T5122': 531, 'T10': 531, 'T120...   \n",
      "14            {'T10250': 50, 'T573': 17, 'T2635': 17}   \n",
      "15  {'T1203': 261, 'T594': 22, 'T4454': 178, 'T273...   \n",
      "16  {'T426': 391, 'T1297': 391, 'T7023': 391, 'T97...   \n",
      "17  {'T911': 153, 'T875': 153, 'T64': 153, 'T2035'...   \n",
      "18  {'T290': 8594, 'T379': 4228, 'T2261': 6110, 'T...   \n",
      "19  {'T873': 81, 'T456': 81, 'T6040': 81, 'T1930':...   \n",
      "20  {'T426': 391, 'T1297': 391, 'T7023': 391, 'T97...   \n",
      "21  {'T992': 15, 'T3': 100, 'T6': 81, 'T6230': 246...   \n",
      "22  {'T231': 31, 'T6182': 31, 'T1252': 31, 'T111':...   \n",
      "23            {'T35': 892, 'T1176': 872, 'T860': 872}   \n",
      "24  {'T85': 362, 'T35': 532, 'T369': 121, 'T2841':...   \n",
      "25  {'T3498': 34, 'T90': 34, 'T1754': 34, 'T989': ...   \n",
      "26  {'T1488': 9, 'T17': 121, 'T688': 27, 'T18': 72...   \n",
      "27  {'T2093': 63, 'T944': 63, 'T2759': 63, 'T74': ...   \n",
      "28  {'T85': 119, 'T1167': 119, 'T35': 528, 'T40': ...   \n",
      "29  {'T174': 430, 'T1559': 78, 'T35': 274, 'T3127'...   \n",
      "\n",
      "                 question_topic_id  \n",
      "0             T6090,T2156,T97,T456  \n",
      "1             T6090,T2156,T97,T456  \n",
      "2             T6090,T2156,T97,T456  \n",
      "3             T6090,T2156,T97,T456  \n",
      "4             T6090,T2156,T97,T456  \n",
      "5                      T42650,T184  \n",
      "6                      T42650,T184  \n",
      "7                      T42650,T184  \n",
      "8                      T42650,T184  \n",
      "9                      T42650,T184  \n",
      "10                     T42650,T184  \n",
      "11                     T42650,T184  \n",
      "12                     T42650,T184  \n",
      "13                     T42650,T184  \n",
      "14                     T42650,T184  \n",
      "15                     T42650,T184  \n",
      "16  T8430,T6916,T7356,T24271,T5817  \n",
      "17  T8430,T6916,T7356,T24271,T5817  \n",
      "18  T8430,T6916,T7356,T24271,T5817  \n",
      "19  T8430,T6916,T7356,T24271,T5817  \n",
      "20                        T35,T992  \n",
      "21                        T35,T992  \n",
      "22                        T35,T992  \n",
      "23                        T35,T992  \n",
      "24                        T35,T992  \n",
      "25                        T35,T992  \n",
      "26                        T35,T992  \n",
      "27                        T35,T992  \n",
      "28                        T35,T992  \n",
      "29                        T35,T992  \n",
      "start making u_topic_scores...\n",
      "               用户id                                               dict\n",
      "0       M1000000382                              {'T13': 0, 'T567': 0}\n",
      "1       M1000000983  {'T6192': 8, 'T1667': 56, 'T2255': 31, 'T1103'...\n",
      "2       M1000020034                                       {'T597': 90}\n",
      "3       M1000022555   {'T1440': 26, 'T608': 26, 'T103': 73, 'T13': 47}\n",
      "4       M1000025214                           {'T86': 226, 'T74': 226}\n",
      "5       M1000031027  {'T49': 24, 'T82': 24, 'T72': 30, 'T42': 30, '...\n",
      "6       M1000043012  {'T4557': 710, 'T543': 506, 'T1655': 277, 'T55...\n",
      "7       M1000047678  {'T7': 10, 'T2': 10, 'T35': 10, 'T252': 10, 'T...\n",
      "8       M1000054663  {'T539': 858, 'T132': 180, 'T272': 439, 'T491'...\n",
      "9       M1000058328        {'T234': 7, 'T101': 7, 'T2': 7, 'T5807': 7}\n",
      "10      M1000063561  {'T37': 920, 'T79364': 29, 'T112': 29, 'T204':...\n",
      "11      M1000071070  {'T664': 13, 'T12751': 13, 'T660': 13, 'T93': ...\n",
      "12      M1000076506  {'T11906': 58, 'T81': 58, 'T6057': 27, 'T29193...\n",
      "13      M1000084325  {'T9825': 55, 'T5448': 55, 'T19624': 55, 'T819...\n",
      "14      M1000094299  {'T42': 23, 'T78': 23, 'T255': 26, 'T131': 23,...\n",
      "15      M1000141697  {'T24': 235, 'T791': 235, 'T417': 9, 'T1346': ...\n",
      "16      M1000158078              {'T48': 213, 'T121': 213, 'T13': 213}\n",
      "17       M100015868  {'T2529': 12, 'T1905': 12, 'T2977': 17, 'T9': ...\n",
      "18      M1000167043  {'T3873': 54, 'T1788': 54, 'T910': 54, 'T52': ...\n",
      "19      M1000192704  {'T9902': 495, 'T388': 495, 'T595': 495, 'T640...\n",
      "20      M1000193118  {'T4873': 10, 'T1966': 24, 'T917': 10, 'T8161'...\n",
      "21      M1000212511  {'T1065': 26, 'T183': 31, 'T112': 31, 'T534': ...\n",
      "22      M1000219408  {'T4897': 44, 'T576': 44, 'T16896': 44, 'T43':...\n",
      "23      M1000228946                         {'T561': 111, 'T2751': 72}\n",
      "24      M1000230783  {'T2003': 25, 'T103': 948, 'T7950': 41, 'T61':...\n",
      "25      M1000258653                   {'T27': 67, 'T3': 67, 'T35': 67}\n",
      "26      M1000274040     {'T153': 700, 'T27': 12, 'T3': 228, 'T52': 31}\n",
      "27       M100030075                 {'T48': 14, 'T708': 14, 'T13': 14}\n",
      "28      M1000322340                           {'T26': 17, 'T1275': 17}\n",
      "29      M1000341131                                     {'T1850': 138}\n",
      "...             ...                                                ...\n",
      "424018   M999652835            {'T898': 15, 'T33874': 15, 'T2084': 29}\n",
      "424019   M999654580                 {'T1998': 4, 'T873': 4, 'T482': 4}\n",
      "424020   M999664630                           {'T5': 291, 'T967': 291}\n",
      "424021   M999672160                      {'T10816': 395, 'T6874': 395}\n",
      "424022   M999673925  {'T85': 216, 'T1184': 35, 'T20634': 35, 'T365'...\n",
      "424023   M999687004  {'T2425': 120, 'T390': 77, 'T161': 77, 'T510':...\n",
      "424024   M999704036  {'T8459': 410, 'T14038': 410, 'T2942': 2902, '...\n",
      "424025   M999726291  {'T177': 16, 'T6': 16, 'T54': 16, 'T3073': 16,...\n",
      "424026   M999731668  {'T9142': 65, 'T1824': 182, 'T12493': 122, 'T6...\n",
      "424027    M99974422  {'T3393': 46, 'T6333': 14, 'T683': 32, 'T719':...\n",
      "424028   M999758583  {'T244': 1911, 'T336': 709, 'T106': 1911, 'T18...\n",
      "424029   M999784603  {'T887': 71, 'T7771': 71, 'T7643': 71, 'T207':...\n",
      "424030   M999786779  {'T611': 660, 'T2509': 208, 'T1059': 507, 'T10...\n",
      "424031   M999787317  {'T2': 81, 'T5': 81, 'T167': 81, 'T22': 81, 'T...\n",
      "424032   M999788674  {'T918': 16, 'T29': 55, 'T354': 16, 'T4825': 1...\n",
      "424033   M999792591  {'T3634': 85, 'T7371': 107, 'T370': 61, 'T312'...\n",
      "424034   M999794397  {'T5673': 416, 'T1680': 416, 'T5871': 416, 'T5...\n",
      "424035   M999813324  {'T407': 61, 'T70': 1079, 'T513': 61, 'T100': ...\n",
      "424036   M999827559                  {'T2': 74, 'T22': 74, 'T488': 74}\n",
      "424037   M999845013                             {'T2': 63, 'T145': 63}\n",
      "424038   M999856979  {'T3651': 558, 'T105': 558, 'T2373': 558, 'T32...\n",
      "424039   M999866056  {'T4': 423, 'T54': 423, 'T24': 423, 'T8': 8081...\n",
      "424040   M999866124  {'T141': 153, 'T19': 215, 'T4006': 21, 'T5550'...\n",
      "424041   M999868450  {'T2911': 57, 'T234': 104, 'T93': 57, 'T6270':...\n",
      "424042   M999895238  {'T1080': 20, 'T6144': 20, 'T1142': 20, 'T619'...\n",
      "424043   M999927399  {'T2846': 18, 'T31976': 18, 'T1401': 18, 'T232...\n",
      "424044   M999931159  {'T10': 137, 'T24': 137, 'T2215': 2066, 'T241'...\n",
      "424045   M999946975  {'T49912': 15, 'T573': 15, 'T374': 1004, 'T212...\n",
      "424046   M999960471  {'T657': 819, 'T6851': 590, 'T6621': 590, 'T96...\n",
      "424047   M999998695                        {'T810': 206, 'T5568': 206}\n",
      "\n",
      "[424048 rows x 2 columns]\n",
      "start making q_topics...\n",
      "                问题id                question_topic_id\n",
      "0        Q1000000315        T3290,T495,T5471,T668,T79\n",
      "1        Q1000002524                            T1909\n",
      "2        Q1000003894                  T616,T227,T3433\n",
      "3        Q1000004883                    T112,T463,T10\n",
      "4        Q1000006560      T8302,T181,T7342,T1169,T259\n",
      "5        Q1000007604               T32200,T8545,T7305\n",
      "6        Q1000008276              T1615,T24,T456,T741\n",
      "7        Q1000010200            T46243,T24,T42,T20351\n",
      "8        Q1000020925                       T9128,T718\n",
      "9        Q1000021185   T3226,T600,T11240,T2361,T12104\n",
      "10       Q1000025766                        T38,T2187\n",
      "11        Q100002777                            T1217\n",
      "12       Q1000030162               T10079,T38,T762,T2\n",
      "13        Q100003566        T18296,T14059,T30048,T184\n",
      "14       Q1000036156                       T905,T4345\n",
      "15       Q1000037403                  T39,T1850,T2091\n",
      "16       Q1000037824                           T12572\n",
      "17       Q1000041691                     T3680,T11711\n",
      "18       Q1000049860             T97,T791,T456,T24091\n",
      "19       Q1000050745                             T893\n",
      "20        Q100005202                       T156,T1249\n",
      "21       Q1000055583                           T18880\n",
      "22       Q1000060170                       T157,T5166\n",
      "23       Q1000060407                        T66,T5810\n",
      "24       Q1000067956             T4894,T263,T561,T558\n",
      "25       Q1000069029                    T20,T15,T1239\n",
      "26       Q1000070819                             T310\n",
      "27       Q1000074692           T9804,T4872,T3098,T983\n",
      "28       Q1000075068                 T1142,T3363,T619\n",
      "29       Q1000077463               T3637,T1083,T17531\n",
      "...              ...                              ...\n",
      "1829870   Q999952782                       T95,T14674\n",
      "1829871    Q99995466         T24,T534,T456,T241,T2215\n",
      "1829872   Q999955973               T670,T240,T109,T26\n",
      "1829873   Q999958198                   T192,T663,T387\n",
      "1829874   Q999960053              T9,T265,T207,T16273\n",
      "1829875   Q999962141                T11058,T590,T6683\n",
      "1829876   Q999962730     T3697,T502,T11,T17382,T16863\n",
      "1829877   Q999967056                T184,T2710,T12843\n",
      "1829878    Q99996827                  T101,T5807,T234\n",
      "1829879   Q999969820                             T827\n",
      "1829880   Q999970938                         T166,T55\n",
      "1829881   Q999972709                  T361,T1122,T255\n",
      "1829882   Q999974398             T71,T371,T196,T27,T7\n",
      "1829883   Q999976398       T1370,T24,T3923,T378,T3838\n",
      "1829884   Q999977222                     T34763,T3218\n",
      "1829885   Q999977345                           T47175\n",
      "1829886   Q999978668     T768,T7004,T5421,T211,T42858\n",
      "1829887   Q999985030                           T13005\n",
      "1829888   Q999985860            T4532,T101,T664,T1293\n",
      "1829889   Q999986062         T970,T132,T272,T1675,T23\n",
      "1829890   Q999986135                         T693,T37\n",
      "1829891   Q999990752                 T28158,T250,T253\n",
      "1829892   Q999991269                T1776,T1044,T9516\n",
      "1829893   Q999992666       T2286,T2751,T839,T62,T2339\n",
      "1829894   Q999993263                       T1054,T463\n",
      "1829895   Q999993281               T7392,T16689,T9030\n",
      "1829896   Q999995156     T34334,T3247,T563,T34632,T27\n",
      "1829897   Q999996608               T11837,T49663,T417\n",
      "1829898   Q999997494  T1519,T12948,T14367,T1283,T2209\n",
      "1829899   Q999999307                   T281,T56,T4879\n",
      "\n",
      "[1829900 rows x 2 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 872658 entries, 0 to 872657\n",
      "Data columns (total 4 columns):\n",
      "uid                  872658 non-null object\n",
      "qid                  872658 non-null object\n",
      "dict                 872658 non-null object\n",
      "question_topic_id    872658 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 33.3+ MB\n",
      "None\n",
      "            uid          qid  \\\n",
      "0     M64135255  Q1493039281   \n",
      "1   M4033621585  Q1493039281   \n",
      "2   M1582438278  Q1493039281   \n",
      "3   M1225946888  Q1493039281   \n",
      "4   M4063235921  Q1493039281   \n",
      "5     M64135255  Q3261378169   \n",
      "6    M104320558  Q3261378169   \n",
      "7    M112479357  Q3261378169   \n",
      "8   M3145063127  Q3261378169   \n",
      "9   M3153261967  Q3261378169   \n",
      "10  M3294926344  Q4151338694   \n",
      "11  M3334924597  Q4151338694   \n",
      "12  M3294926344   Q827712338   \n",
      "13   M971763740   Q827712338   \n",
      "14   M983129880   Q827712338   \n",
      "15  M4249194169   Q827712338   \n",
      "16  M3446346696   Q827712338   \n",
      "17  M3187295856   Q827712338   \n",
      "18  M1705333001   Q827712338   \n",
      "19   M837010899   Q827712338   \n",
      "20  M3398637577   Q827712338   \n",
      "21  M3577162321   Q827712338   \n",
      "22  M2021259079   Q827712338   \n",
      "23   M103931090   Q827712338   \n",
      "24  M2209060459   Q827712338   \n",
      "25  M4025653532   Q827712338   \n",
      "26  M1421400472   Q827712338   \n",
      "27  M2840942075   Q827712338   \n",
      "28  M1456054995   Q827712338   \n",
      "29  M2358323882   Q827712338   \n",
      "\n",
      "                                                 dict  \\\n",
      "0                          {'T93': 2007, 'T3887': 45}   \n",
      "1   {'T808': 43, 'T1751': 64, 'T42': 64, 'T456': 9...   \n",
      "2   {'T660': 464, 'T101': 464, 'T234': 464, 'T93':...   \n",
      "3   {'T674': 74, 'T2': 74, 'T22': 74, 'T36': 74, '...   \n",
      "4   {'T11431': 514, 'T101': 514, 'T1469': 514, 'T2...   \n",
      "5                          {'T93': 2007, 'T3887': 45}   \n",
      "6   {'T45384': 23, 'T93': 124, 'T101': 104, 'T664'...   \n",
      "7   {'T19020': 1145, 'T8466': 1145, 'T101': 1126, ...   \n",
      "8   {'T3019': 1734, 'T234': 2237, 'T47442': 503, '...   \n",
      "9                               {'T101': 6, 'T93': 6}   \n",
      "10  {'T7494': 84, 'T18700': 84, 'T61': 84, 'T12519...   \n",
      "11  {'T25690': 47, 'T983': 166, 'T2239': 166, 'T27...   \n",
      "12  {'T7494': 84, 'T18700': 84, 'T61': 84, 'T12519...   \n",
      "13  {'T1505': 644, 'T542': 644, 'T66': 742, 'T24':...   \n",
      "14  {'T1740': 230, 'T461': 230, 'T1482': 230, 'T16...   \n",
      "15  {'T4520': 72, 'T183': 9, 'T184': 9, 'T212': 83...   \n",
      "16  {'T4324': 909, 'T24': 1024, 'T6608': 1024, 'T3...   \n",
      "17  {'T43': 8, 'T191': 8, 'T684': 41, 'T596': 75, ...   \n",
      "18  {'T7711': 736, 'T842': 736, 'T222': 736, 'T684...   \n",
      "19  {'T684': 148, 'T2': 148, 'T44': 148, 'T266': 148}   \n",
      "20  {'T2': 39, 'T9': 16, 'T2482': 16, 'T53': 16, '...   \n",
      "21  {'T389': 784, 'T66': 784, 'T17389': 784, 'T459...   \n",
      "22  {'T1345': 1282, 'T184': 59, 'T1751': 88, 'T52'...   \n",
      "23  {'T11': 835, 'T1215': 835, 'T948': 835, 'T66':...   \n",
      "24  {'T97': 44, 'T2975': 44, 'T4086': 44, 'T7961':...   \n",
      "25  {'T3140': 20, 'T66': 48, 'T184': 48, 'T705': 4...   \n",
      "26  {'T1374': 907, 'T4328': 23, 'T391': 304, 'T184...   \n",
      "27  {'T5539': 1398, 'T1179': 1398, 'T716': 1398, '...   \n",
      "28  {'T316': 6, 'T161': 6, 'T390': 6, 'T141': 6, '...   \n",
      "29  {'T133': 103, 'T266': 103, 'T684': 103, 'T29':...   \n",
      "\n",
      "                question_topic_id  \n",
      "0              T101,T58775,T45384  \n",
      "1              T101,T58775,T45384  \n",
      "2              T101,T58775,T45384  \n",
      "3              T101,T58775,T45384  \n",
      "4              T101,T58775,T45384  \n",
      "5              T45384,T47442,T101  \n",
      "6              T45384,T47442,T101  \n",
      "7              T45384,T47442,T101  \n",
      "8              T45384,T47442,T101  \n",
      "9              T45384,T47442,T101  \n",
      "10  T2194,T1576,T2239,T9282,T5655  \n",
      "11  T2194,T1576,T2239,T9282,T5655  \n",
      "12                           T684  \n",
      "13                           T684  \n",
      "14                           T684  \n",
      "15                           T684  \n",
      "16                           T684  \n",
      "17                           T684  \n",
      "18                           T684  \n",
      "19                           T684  \n",
      "20                           T684  \n",
      "21                           T684  \n",
      "22                           T684  \n",
      "23                           T684  \n",
      "24                           T684  \n",
      "25                           T684  \n",
      "26                           T684  \n",
      "27                           T684  \n",
      "28                           T684  \n",
      "29                           T684  \n"
     ]
    }
   ],
   "source": [
    "train_for_save1 = extract_user_topic_word_count(train_for_save, train_label, 'train')\n",
    "test_for_save1 = extract_user_topic_word_count(test_for_save, test, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理dict列的空值\n",
    "train_for_save1 = train_for_save1[train_for_save1['dict']!=-1].reset_index().drop('index', axis=1)\n",
    "test_for_save1 = test_for_save1[test_for_save1['dict']!=-1].reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_for_save1['qid'], train_for_save1['question_topic_id']\n",
    "del test_for_save1['qid'], test_for_save1['question_topic_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('u_topic_a_word_count_dict_train.pkl', 'wb') as file:\n",
    "    pickle.dump(train_for_save1, file)\n",
    "with open('u_topic_a_word_count_dict_val.pkl', 'wb') as file:\n",
    "    pickle.dump(test_for_save1, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算用户累计回答字数最多的话题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 618787 entries, 0 to 618786\n",
      "Data columns (total 2 columns):\n",
      "uid     618787 non-null object\n",
      "dict    618787 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 9.4+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 424048 entries, 0 to 424047\n",
      "Data columns (total 2 columns):\n",
      "uid     424048 non-null object\n",
      "dict    424048 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 6.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train_user_topic_zishu = train_for_save1.drop_duplicates(subset=['uid'], keep='first', inplace=False).reset_index().drop('index', axis=1)\n",
    "test_user_topic_zishu = test_for_save1.drop_duplicates(subset=['uid'], keep='first', inplace=False).reset_index().drop('index', axis=1)\n",
    "print(train_user_topic_zishu.info())\n",
    "print(test_user_topic_zishu.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#需要对topic id进行统一编码\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "topic = pd.read_csv(os.path.join(data_path, 'topic_vectors_64d.txt'), \n",
    "                          names=['id', 'embed'], sep='\\t')\n",
    "topic_list = topic['id']\n",
    "topic_list = topic_list.append(pd.Series(['-1']))\n",
    "encoder.fit(topic_list)\n",
    "#data[feat] = encoder.transform(data[feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_wordcount_topic(d):\n",
    "    if len(d) == 0:\n",
    "        return '-1'\n",
    "    return list(d.keys())[np.argmax(list(d.values()))]\n",
    "\n",
    "def get_interest_values(d):\n",
    "    if len(d) == 0:\n",
    "        return [0]\n",
    "    return list(d.values())\n",
    "\n",
    "def get_answer_topics(d):\n",
    "    if len(d) == 0:\n",
    "        return '-1'\n",
    "    return list(d.keys())\n",
    "\n",
    "def get_user_topic_zishu(train_user_topic_zishu):\n",
    "    #train_user_topic_zishu['u_t_most_countword_id'] = train_user_topic_zishu['dict'].apply(most_wordcount_topic)\n",
    "    \n",
    "    #train_user_topic_zishu['u_t_most_countword_id'] = encoder.transform(train_user_topic_zishu['u_t_most_countword_id'])\n",
    "    \n",
    "    train_user_topic_zishu['u_t_a_count'] = train_user_topic_zishu['dict'].apply(len)\n",
    "\n",
    "    train_user_topic_zishu['u_t_countword_values'] = train_user_topic_zishu['dict'].apply(get_interest_values)\n",
    "    #train_user_topic_zishu['u_t_min_countword_values'] = train_user_topic_zishu['u_t_countword_values'].apply(np.min)\n",
    "    train_user_topic_zishu['u_t_max_countword_values'] = train_user_topic_zishu['u_t_countword_values'].apply(np.max)\n",
    "    #train_user_topic_zishu['u_t_mean_countword_values'] = train_user_topic_zishu['u_t_countword_values'].apply(np.mean)\n",
    "    train_user_topic_zishu['u_t_std_countword_values'] = train_user_topic_zishu['u_t_countword_values'].apply(np.std)\n",
    "    \n",
    "    del train_user_topic_zishu['dict']#, train_user_topic_zishu['u_t_countword_values']\n",
    "\n",
    "    return train_user_topic_zishu\n",
    "\n",
    "def get_user_topic_answered(train_user_topic_zishu):\n",
    "    #train_user_topic_zishu['u_t_most_countword_id'] = train_user_topic_zishu['dict'].apply(most_wordcount_topic)\n",
    "    \n",
    "    #train_user_topic_zishu['u_t_most_countword_id'] = encoder.transform(train_user_topic_zishu['u_t_most_countword_id'])\n",
    "    \n",
    "    #train_user_topic_zishu['u_t_a_count'] = train_user_topic_zishu['dict'].apply(len)\n",
    "\n",
    "    train_user_topic_zishu['u_t_answered_topics'] = train_user_topic_zishu['dict'].apply(get_answer_topics)\n",
    "    #train_user_topic_zishu['u_t_min_countword_values'] = train_user_topic_zishu['u_t_countword_values'].apply(np.min)\n",
    "    #train_user_topic_zishu['u_t_max_countword_values'] = train_user_topic_zishu['u_t_countword_values'].apply(np.max)\n",
    "    #train_user_topic_zishu['u_t_mean_countword_values'] = train_user_topic_zishu['u_t_countword_values'].apply(np.mean)\n",
    "    #train_user_topic_zishu['u_t_std_countword_values'] = train_user_topic_zishu['u_t_countword_values'].apply(np.std)\n",
    "    \n",
    "    del train_user_topic_zishu['dict']#, train_user_topic_zishu['u_t_countword_values']\n",
    "\n",
    "    return train_user_topic_zishu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_topic_zishu1 = get_user_topic_zishu(train_user_topic_zishu)\n",
    "test_user_topic_zishu1 = get_user_topic_zishu(test_user_topic_zishu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_topic_answered = get_user_topic_answered(train_user_topic_zishu)\n",
    "test_user_topic_answered = get_user_topic_answered(test_user_topic_zishu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           uid                                u_t_answered_topics\n",
      "0  M2317670257  [T10, T10186, T42650, T184, T1203, T4454, T564...\n",
      "1   M893603481  [T4385, T2863, T70, T9160, T1018, T1749, T97, ...\n",
      "2  M1699039805  [T2, T47, T22, T3, T950, T566, T52, T1389, T65...\n",
      "3  M3147978248                     [T208, T5339, T927, T271, T11]\n",
      "4   M866789582                            [T32, T81, T524, T2522]\n",
      "           uid                                u_t_answered_topics\n",
      "0    M64135255                                       [T93, T3887]\n",
      "1  M4033621585  [T808, T1751, T42, T456, T241, T534, T86, T111...\n",
      "2  M1582438278                            [T660, T101, T234, T93]\n",
      "3  M1225946888  [T674, T2, T22, T36, T31, T492, T224, T45384, ...\n",
      "4  M4063235921                  [T11431, T101, T1469, T234, T150]\n"
     ]
    }
   ],
   "source": [
    "print(train_user_topic_answered.head())\n",
    "print(test_user_topic_answered.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                uid u_t_answered_topics\n",
      "0       M2317670257                 T10\n",
      "0       M2317670257              T10186\n",
      "0       M2317670257              T42650\n",
      "0       M2317670257                T184\n",
      "0       M2317670257               T1203\n",
      "0       M2317670257               T4454\n",
      "0       M2317670257               T5649\n",
      "0       M2317670257               T5122\n",
      "1        M893603481               T4385\n",
      "1        M893603481               T2863\n",
      "1        M893603481                 T70\n",
      "1        M893603481               T9160\n",
      "1        M893603481               T1018\n",
      "1        M893603481               T1749\n",
      "1        M893603481                 T97\n",
      "1        M893603481                T100\n",
      "1        M893603481               T1813\n",
      "1        M893603481                T407\n",
      "1        M893603481               T1262\n",
      "1        M893603481               T7007\n",
      "1        M893603481              T41931\n",
      "1        M893603481               T3460\n",
      "1        M893603481              T59710\n",
      "1        M893603481               T7019\n",
      "1        M893603481               T1278\n",
      "1        M893603481               T8842\n",
      "1        M893603481               T5083\n",
      "1        M893603481              T20104\n",
      "1        M893603481               T8532\n",
      "1        M893603481               T6476\n",
      "...             ...                 ...\n",
      "618783  M3005292423                T349\n",
      "618783  M3005292423                T208\n",
      "618783  M3005292423                T271\n",
      "618783  M3005292423                T344\n",
      "618783  M3005292423                 T12\n",
      "618783  M3005292423                  T9\n",
      "618783  M3005292423                 T47\n",
      "618783  M3005292423                T711\n",
      "618783  M3005292423               T1589\n",
      "618783  M3005292423                T125\n",
      "618783  M3005292423                T302\n",
      "618783  M3005292423                T395\n",
      "618783  M3005292423                T561\n",
      "618784  M3901237247                 T82\n",
      "618784  M3901237247                 T49\n",
      "618784  M3901237247                 T34\n",
      "618784  M3901237247               T7643\n",
      "618784  M3901237247               T7771\n",
      "618784  M3901237247                 T27\n",
      "618784  M3901237247                 T52\n",
      "618784  M3901237247                T256\n",
      "618785  M3460995857               T1143\n",
      "618785  M3460995857               T3353\n",
      "618786   M746829203                T486\n",
      "618786   M746829203                  T3\n",
      "618786   M746829203               T7594\n",
      "618786   M746829203                 T12\n",
      "618786   M746829203                 T25\n",
      "618786   M746829203                 T83\n",
      "618786   M746829203                 T42\n",
      "\n",
      "[8969146 rows x 2 columns]\n",
      "Memory usage of dataframe is 205.29 MB\n",
      "Memory usage after optimization is: 205.29 MB\n",
      "Decreased by 0.0%\n",
      "           uid u_t_answered_topics\n",
      "0  M2317670257                 T10\n",
      "0  M2317670257              T10186\n",
      "0  M2317670257              T42650\n",
      "0  M2317670257                T184\n",
      "0  M2317670257               T1203\n",
      "0  M2317670257               T4454\n",
      "0  M2317670257               T5649\n",
      "0  M2317670257               T5122\n",
      "1   M893603481               T4385\n",
      "1   M893603481               T2863\n"
     ]
    }
   ],
   "source": [
    "def get_user_sparse_atopic(answer_qestion_info, etype):\n",
    "    answer_qestion_info1 = answer_qestion_info.reindex(answer_qestion_info.index.repeat(answer_qestion_info.u_t_answered_topics.str.len())).assign(u_t_answered_topics=np.concatenate(answer_qestion_info.u_t_answered_topics.values))\n",
    "    #answer_qestion_info1 = answer_qestion_info['u_t_answered_topics'].str.split(',', expand=True)#.stack()\n",
    "    print(answer_qestion_info1)\n",
    "    answer_qestion_info1 = answer_qestion_info1.rename({' u_t_answered_topics':'u_t_answered_single_topic'})\n",
    "    #answer_qestion_info = answer_qestion_info.join(answer_qestion_info1)\n",
    "    #print(answer_qestion_info.head(10))\n",
    "    del answer_qestion_info\n",
    "    answer_qestion_info1 = reduce_mem_usage(answer_qestion_info1)\n",
    "    print(answer_qestion_info1.head(10))\n",
    "    \n",
    "    with open(etype+'user_topic.pkl', 'wb') as file:\n",
    "        pickle.dump(answer_qestion_info1, file)   \n",
    "        \n",
    "    return answer_qestion_info1\n",
    "\n",
    "train_user_topic_answered1 = get_user_sparse_atopic(train_user_topic_answered, 'train')\n",
    "test_user_topic_answered1 = get_user_sparse_atopic(test_user_topic_answered, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_hot_for_user(answer_qestion_info, train_label, etype):\n",
    "    topic_list = answer_qestion_info[['uid','u_t_answered_topics']]\n",
    "    topic_list['topic_count'] = topic_list['user_topic_id'].map(topic_list['user_topic_id'].value_counts().astype(int))\n",
    "    topic_list = topic_list[['user_topic_id','topic_count']].drop_duplicates(inplace=False)\n",
    "    del answer_qestion_info\n",
    "    \n",
    "    print(topic_list.head(30))\n",
    "    with open(etype+'topic_count.pkl', 'wb') as file:\n",
    "        pickle.dump(topic_count, file)\n",
    "    return topic_list\n",
    "\n",
    "def merge_que_topic_count(topic_count, etype):\n",
    "    \n",
    "    with open(etype+'question_topic.pkl', 'rb') as file:\n",
    "        question_topic = pickle.load(file)\n",
    "    print(question_topic.head(5))\n",
    "    \n",
    "    question_label = pd.merge(question_topic, topic_count, left_on='question_topic_id', right_on='user_topic_id', how='inner').drop('question_topic_id',axis=1)\n",
    "    question_label['q_t_a_count_sum'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_count'].agg('sum').astype(int))\n",
    "    question_label['q_t_a_count_mean'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_count'].agg('mean'))\n",
    "    question_label['q_t_a_count_max'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_count'].agg('max').astype(int))\n",
    "    question_label['q_t_a_count_std'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_count'].agg('std'))\n",
    "    question_label['q_t_a_count_min'] = question_label['问题id'].map(question_label.groupby('问题id')['topic_count'].agg('min').astype(int))\n",
    "    \n",
    "    question_label = question_label[['问题id','q_t_a_count_sum','q_t_a_count_mean','q_t_a_count_max','q_t_a_count_min','q_t_a_count_std']].drop_duplicates(inplace=False)\n",
    "    \n",
    "    with open(etype+'que_topic_count.pkl', 'wb') as file:\n",
    "        pickle.dump(question_label, file)\n",
    "    \n",
    "    return question_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 2593669"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('u_topic_answered_train.pkl', 'wb') as file:\n",
    "    pickle.dump(train_user_topic_zishu1, file)\n",
    "with open('u_topic_answered_val.pkl', 'wb') as file:\n",
    "    pickle.dump(test_user_topic_zishu1, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('u_topic_a_zishu_train.pkl', 'wb') as file:\n",
    "    pickle.dump(train_user_topic_zishu1, file)\n",
    "with open('u_topic_a_zishu_val.pkl', 'wb') as file:\n",
    "    pickle.dump(test_user_topic_zishu1, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算用户累计得到最多赞的话题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-351:\n",
      "Process ForkPoolWorker-370:\n",
      "Process ForkPoolWorker-350:\n",
      "Process ForkPoolWorker-358:\n",
      "Process ForkPoolWorker-363:\n",
      "Process ForkPoolWorker-346:\n",
      "Process ForkPoolWorker-344:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "Process ForkPoolWorker-354:\n",
      "Process ForkPoolWorker-362:\n",
      "Process ForkPoolWorker-360:\n",
      "Process ForkPoolWorker-365:\n",
      "Process ForkPoolWorker-347:\n",
      "Process ForkPoolWorker-364:\n",
      "Process ForkPoolWorker-345:\n",
      "Process ForkPoolWorker-357:\n",
      "Process ForkPoolWorker-367:\n",
      "Process ForkPoolWorker-348:\n",
      "Process ForkPoolWorker-349:\n",
      "Process ForkPoolWorker-353:\n",
      "Process ForkPoolWorker-356:\n",
      "Process ForkPoolWorker-343:\n",
      "Process ForkPoolWorker-332:\n",
      "Process ForkPoolWorker-337:\n",
      "Process ForkPoolWorker-361:\n",
      "Process ForkPoolWorker-339:\n",
      "Process ForkPoolWorker-340:\n",
      "Process ForkPoolWorker-342:\n",
      "Process ForkPoolWorker-366:\n",
      "Process ForkPoolWorker-359:\n",
      "Process ForkPoolWorker-352:\n",
      "Process ForkPoolWorker-335:\n",
      "Process ForkPoolWorker-331:\n",
      "Process ForkPoolWorker-341:\n",
      "Process ForkPoolWorker-338:\n",
      "Process ForkPoolWorker-369:\n",
      "Process ForkPoolWorker-334:\n",
      "Process ForkPoolWorker-368:\n",
      "Process ForkPoolWorker-333:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-336:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-355:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/jiyangguang/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import gc\n",
    "\n",
    "# 分割 df，方便多进程跑\n",
    "def split_df(df, n):\n",
    "    chunk_size = int(np.ceil(len(df) / n))\n",
    "    return [df[i*chunk_size:(i+1)*chunk_size] for i in range(n)]\n",
    "\n",
    "def gc_mp(pool, ret, chunk_list):\n",
    "    del pool\n",
    "    for r in ret:\n",
    "        del r\n",
    "    del ret\n",
    "    for cl in chunk_list:\n",
    "        del cl\n",
    "    del chunk_list\n",
    "    gc.collect()\n",
    "    \n",
    "# 用户感兴趣topic和问题 topic的交集\n",
    "def process(df):\n",
    "    return df.apply(lambda row: list(set(row['dict'].keys()) & set(row['question_topic_id'])),axis=1)\n",
    "\n",
    "pool = mp.Pool()\n",
    "chunk_list = split_df(train_for_save1, 100)\n",
    "ret = pool.map(process, chunk_list)\n",
    "train_for_save1['topic_answer_intersection'] = pd.concat(ret)\n",
    "gc_mp(pool, ret, chunk_list)\n",
    "\n",
    "# 用户感兴趣topic和问题 topic的交集的兴趣值\n",
    "def process(df):\n",
    "    return df.apply(lambda row: [row['dict'][t] for t in row['topic_answer_intersection']],axis=1)\n",
    "\n",
    "pool = mp.Pool()\n",
    "chunk_list = split_df(train_for_save1, 100)\n",
    "ret = pool.map(process, chunk_list)\n",
    "train_for_save1['topic_answer_intersection_values'] = pd.concat(ret)\n",
    "gc_mp(pool, ret, chunk_list)\n",
    "\n",
    "# 交集topic计数\n",
    "train_for_save1['num_topic_answer_intersection'] = train_for_save1['topic_answer_intersection'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [uid, qid, dict, question_topic_id, topic_answer_intersection, topic_answer_intersection_values, num_topic_answer_intersection]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(train_for_save1[train_for_save1['num_topic_answer_intersection']!=0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
