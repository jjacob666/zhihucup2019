{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lightGBM version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "import logging\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "data_path = '../../../data'\n",
    "\n",
    "answer_info = pd.read_csv(os.path.join(data_path, 'answer_info_0926.txt'), header=None, sep='\\t')\n",
    "#train1 = pd.read_csv(os.path.join(data_path, 'invite_info_0926.txt'), header=None, sep='\\t')\n",
    "\n",
    "#train1.columns = ['问题id', '用户id', '邀请创建时间','是否回答']\n",
    "answer_info.columns = ['回答id', '问题id', '用户id', '回答创建时间', '回答内容的单字编码序列', '回答内容的切词编码序列', '回答是否被标优', '回答是否被推荐', '回答是否被收入圆桌', '是否包含图片', '是否包含视频', '回答字数', '点赞数', '取赞数', '评论数', '收藏数', '感谢数', '举报数', '没有帮助数', '反对数']\n",
    "print(answer_info.head(5))\n",
    "answer_info['回答创建时间-day'] = answer_info['回答创建时间'].apply(lambda x:x.split('-')[0].split('D')[1]).astype(int)\n",
    "\n",
    "inv_info = pd.read_csv(os.path.join(data_path, 'invite_info_0926.txt'), header=None, sep='\\t')\n",
    "inv_info.columns = ['问题id', '用户id', '邀请创建时间','是否回答']\n",
    "inv_info['邀请创建时间-day'] = inv_info['邀请创建时间'].apply(lambda x:x.split('-')[0].split('D')[1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = '../3results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = set([1,2,3,4])\n",
    "b= set([2,3,4,5])\n",
    "print(a.difference(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_quest1(train_label, answer_info, inv_info, end_day):\n",
    "    #question_in_answer = set(answer_info[answer_info['回答创建时间-day']<=end_day]['问题id'])\n",
    "    question_in_inv = set(inv_info[inv_info['邀请创建时间-day']<=end_day]['问题id'])\n",
    "    #old_question = list(question_in_answer.union(question_in_inv))\n",
    "    old_question = question_in_inv\n",
    "    \n",
    "    question_in_label = set(train_label['qid'])\n",
    "    question_all = list(question_in_label.union(old_question))\n",
    "    new_question = list(question_in_label.difference(old_question))\n",
    "    \n",
    "    print(len(old_question), len(new_question))\n",
    "\n",
    "    return old_question, new_question\n",
    "\n",
    "def get_new_quest(train_label,answer_info, inv_info, end_day):\n",
    "    #question_in_answer = set(answer_info[answer_info['回答创建时间-day']<=end_day]['问题id'])\n",
    "    question_in_inv = set(inv_info[inv_info['邀请创建时间-day']<=end_day]['问题id'])\n",
    "    #old_question = list(question_in_answer.union(question_in_inv))\n",
    "    old_question = question_in_inv\n",
    "    \n",
    "    all_question = set(train_label['qid'])\n",
    "    new_question = list(all_question.difference(old_question))\n",
    "    \n",
    "    print(len(old_question), len(new_question))\n",
    "\n",
    "    return old_question, new_question\n",
    "\n",
    "\n",
    "def get_new_users1(train_label, answer_info, inv_info, end_day):\n",
    "    users_in_answer = set(answer_info[answer_info['回答创建时间-day']<=end_day]['用户id'])\n",
    "    users_in_inv = set(inv_info[inv_info['邀请创建时间-day']<=end_day]['用户id'])\n",
    "    old_users = set(users_in_answer.union(users_in_inv))\n",
    "    \n",
    "    users_in_label = set(train_label['uid'])\n",
    "    users_all = set(users_in_label.union(old_users))\n",
    "    new_users = set(users_in_label.difference(old_users))\n",
    "    \n",
    "    print(len(old_users), len(new_users))\n",
    "    \n",
    "    return old_users, new_users\n",
    "\n",
    "def get_new_users(train_label, answer_info, inv_info, end_day):\n",
    "     #question_in_answer = set(answer_info[answer_info['回答创建时间-day']<=end_day]['问题id'])\n",
    "    user_in_inv = set(inv_info[inv_info['邀请创建时间-day']<=end_day]['用户id'])\n",
    "    #old_question = list(question_in_answer.union(question_in_inv))\n",
    "    old_user = user_in_inv\n",
    "    \n",
    "    all_user = set(train_label['uid'])\n",
    "    new_user = list(all_user.difference(old_user))\n",
    "    \n",
    "    print(len(old_user), len(new_user))\n",
    "\n",
    "    return old_user, new_user\n",
    "\n",
    "def train_for4(train_all, test, train_name, sub_test, feature_cols):\n",
    "    print(\"****************start\"+train_name+\"****************\")\n",
    "    \n",
    "    if train_name == 'ouoq':\n",
    "        print(feature_cols)\n",
    "        print(len(feature_cols))\n",
    "        \n",
    "    elif train_name == 'nuoq':\n",
    "        feature_cols = [x for x in feature_cols if x not in ('u_a_last_daytime', 'u_inv_count', 'u_inv_mean', 'u_inv_std', 'u_inv_sum', 'u_t_most_countword_id',\n",
    "                                                            'u_a_i_diffday_max', 'u_a_i_diffday_mean', 'u_a_i_diffday_min', 'u_a_i_diffday_sum', 'u_a_i_diffhour_max', 'u_a_i_diffhour_mean', \n",
    "                                                             'u_a_i_diffhour_min', 'u_a_i_diffhour_sum', 'u_diff_qa_days_max', 'u_diff_qa_days_max_last14day', 'u_diff_qa_days_max_last3day', \n",
    "                                                             'u_diff_qa_days_max_last7day', 'u_diff_qa_days_mean', 'u_diff_qa_days_mean_last14day', 'u_diff_qa_days_mean_last3day', \n",
    "                                                             'u_diff_qa_days_mean_last7day', 'u_diff_qa_days_sum', 'u_diff_qa_days_sum_last14day', 'u_diff_qa_days_sum_last3day', 'u_diff_qa_days_sum_last7day',\n",
    "                                                             'u_a_most_hour', 'u_a_most_hour_last1week', 'u_a_most_hour_last2week', 'u_a_most_hour_last3day', 'u_a_most_wkday',\n",
    "                                                             'u_a_num', 'u_a_num_last14', 'u_a_num_last3', 'u_a_num_last7' ) ]\n",
    "        print(feature_cols)\n",
    "        \n",
    "    elif train_name == 'nunq':\n",
    "        feature_cols = [x for x in feature_cols if x not in ('u_a_last_daytime', 'u_inv_count', 'u_inv_mean', 'u_inv_std', 'u_inv_sum', 'u_t_most_countword_id',\n",
    "                                                            'u_a_i_diffday_max', 'u_a_i_diffday_mean', 'u_a_i_diffday_min', 'u_a_i_diffday_sum', 'u_a_i_diffhour_max', 'u_a_i_diffhour_mean', \n",
    "                                                             'u_a_i_diffhour_min', 'u_a_i_diffhour_sum', 'u_diff_qa_days_max', 'u_diff_qa_days_max_last14day', 'u_diff_qa_days_max_last3day', \n",
    "                                                             'u_diff_qa_days_max_last7day', 'u_diff_qa_days_mean', 'u_diff_qa_days_mean_last14day', 'u_diff_qa_days_mean_last3day', \n",
    "                                                             'u_diff_qa_days_mean_last7day', 'u_diff_qa_days_sum', 'u_diff_qa_days_sum_last14day', 'u_diff_qa_days_sum_last3day', 'u_diff_qa_days_sum_last7day',\n",
    "                                                             'u_a_most_hour', 'u_a_most_hour_last1week', 'u_a_most_hour_last2week', 'u_a_most_hour_last3day', 'u_a_most_wkday',\n",
    "                                                             'u_a_num', 'u_a_num_last14', 'u_a_num_last3', 'u_a_num_last7' ) ]\n",
    "        \n",
    "        feature_cols = [x for x in feature_cols if x not in ('q_inv_count', 'q_inv_mean', 'q_inv_std', 'q_inv_sum', \n",
    "                                                            'q_ans_count', 'q_diff_qa_days_max', 'q_diff_qa_days_mean', 'q_diff_qa_days_sum', 'q_has_img_max', 'q_has_img_mean', \n",
    "                                                             'q_has_img_sum', 'q_has_video_max', 'q_has_video_mean', 'q_has_video_sum', 'q_is_dest_max', 'q_is_dest_mean', 'q_is_dest_sum', \n",
    "                                                             'q_is_good_max', 'q_is_good_mean', 'q_is_good_sum', 'q_is_rec_max', 'q_is_rec_mean', 'q_is_rec_sum', \n",
    "                                                             'q_reci_cheer_max', 'q_reci_cheer_mean', 'q_reci_cheer_sum', 'q_reci_comment_max', 'q_reci_comment_mean', 'q_reci_comment_sum', \n",
    "                                                             'q_reci_dis_max', 'q_reci_dis_mean', 'q_reci_dis_sum', 'q_reci_mark_max', 'q_reci_mark_mean', 'q_reci_mark_sum', \n",
    "                                                             'q_reci_no_help_max', 'q_reci_no_help_mean', 'q_reci_no_help_sum', 'q_reci_tks_max', 'q_reci_tks_mean', 'q_reci_tks_sum', \n",
    "                                                             'q_reci_uncheer_max', 'q_reci_uncheer_mean', 'q_reci_uncheer_sum', \n",
    "                                                             'q_reci_xxx_max', 'q_reci_xxx_mean', 'q_reci_xxx_sum', 'q_word_count_max', 'q_word_count_mean', 'q_word_count_sum')]    \n",
    "        print(feature_cols)\n",
    "        \n",
    "    elif train_name == 'ounq':\n",
    "        feature_cols = [x for x in feature_cols if x not in ('q_inv_count', 'q_inv_mean', 'q_inv_std', 'q_inv_sum', \n",
    "                                                            'q_ans_count', 'q_diff_qa_days_max', 'q_diff_qa_days_mean', 'q_diff_qa_days_sum', 'q_has_img_max', 'q_has_img_mean', \n",
    "                                                             'q_has_img_sum', 'q_has_video_max', 'q_has_video_mean', 'q_has_video_sum', 'q_is_dest_max', 'q_is_dest_mean', 'q_is_dest_sum', \n",
    "                                                             'q_is_good_max', 'q_is_good_mean', 'q_is_good_sum', 'q_is_rec_max', 'q_is_rec_mean', 'q_is_rec_sum', \n",
    "                                                             'q_reci_cheer_max', 'q_reci_cheer_mean', 'q_reci_cheer_sum', 'q_reci_comment_max', 'q_reci_comment_mean', 'q_reci_comment_sum', \n",
    "                                                             'q_reci_dis_max', 'q_reci_dis_mean', 'q_reci_dis_sum', 'q_reci_mark_max', 'q_reci_mark_mean', 'q_reci_mark_sum', \n",
    "                                                             'q_reci_no_help_max', 'q_reci_no_help_mean', 'q_reci_no_help_sum', 'q_reci_tks_max', 'q_reci_tks_mean', 'q_reci_tks_sum', \n",
    "                                                             'q_reci_uncheer_max', 'q_reci_uncheer_mean', 'q_reci_uncheer_sum', \n",
    "                                                             'q_reci_xxx_max', 'q_reci_xxx_mean', 'q_reci_xxx_sum', 'q_word_count_max', 'q_word_count_mean', 'q_word_count_sum')]        \n",
    "        print(feature_cols)    \n",
    "        \n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    train_all = shuffle(train_all)\n",
    "    X_train_all = train_all[feature_cols]\n",
    "    y_train_all = train_all['label']\n",
    "\n",
    "    fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=80)\n",
    "    #scores=[]\n",
    "    \n",
    "    for index, (train_idx, val_idx) in enumerate(fold.split(X=X_train_all, y=y_train_all)):\n",
    "        break\n",
    "    X_train, X_val, y_train, y_val = X_train_all.iloc[train_idx][feature_cols], X_train_all.iloc[val_idx][feature_cols], \\\n",
    "                                 y_train_all.iloc[train_idx], \\\n",
    "                                 y_train_all.iloc[val_idx]\n",
    "    model_lgb = LGBMClassifier(n_estimators=2000, n_jobs=-1, objective='binary', seed=1500, silent=True)  #scale_pos_weight=2.5 when data=data_all\n",
    "    \n",
    "    model_lgb.fit(X_train, y_train,\n",
    "              eval_metric=['logloss', 'auc'],\n",
    "              eval_set=[(X_val, y_val)],\n",
    "              early_stopping_rounds=300)\n",
    "    \n",
    "    del X_train_all\n",
    "    print(pd.DataFrame({\n",
    "            'column': feature_cols,\n",
    "            'importance': model_lgb.feature_importances_\n",
    "                    }).sort_values(by='importance', ascending=False))\n",
    "\n",
    "    \n",
    "    model_lgb.booster_.save_model('model'+train_name+'.txt')\n",
    "    #assert sub_size == test_res.shape[0]\n",
    "    sub_test['label'] = model_lgb.predict_proba(test[feature_cols])[:, 1]\n",
    "    sub_test.to_hdf('model_saved/sub_test'+train_name+'.hd5', key='data')\n",
    "    print(\"**********************end\"+train_name+\"*******************************\")\n",
    "\n",
    "    return sub_test ###['uid','qid','time','label','index']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 减少内存占用\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            \n",
    "            if str(col_type)[:5] == 'float':\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "feature_path = '../1pre_features'\n",
    "with open(os.path.join(feature_path, 'que_topic_all_count_train.pkl'), 'rb') as file:\n",
    "    train_q_t_c = pickle.load(file)\n",
    "    train_q_t_c = reduce_mem_usage(train_q_t_c)\n",
    "print(train_q_t_c.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_q_t_c.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(train_q_t_c.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(feature_path, 'user_topic_all_count_train.pkl'), 'rb') as file:\n",
    "    train_u_t_c = pickle.load(file)[['uid', 'u_t_a_count_sum', 'u_t_a_count_mean','u_t_a_num']]#, 'u_t_a_count_max', 'u_t_a_count_std', 'u_t_a_count_min']]\n",
    "    train_u_t_c = reduce_mem_usage(train_u_t_c)\n",
    "print(train_u_t_c.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#train1 = pd.read_csv(os.path.join(data_path, 'invite_info_0926.txt'), header=None, sep='\\t')\n",
    "test1 = pd.read_csv(os.path.join(data_path, 'invite_info_evaluate_1_0926.txt'), header=None, sep='\\t')\n",
    "test1.columns = ['qid', 'uid', 'time']\n",
    "\n",
    "feature_path = '../1pre_features'\n",
    "data = pd.read_hdf(os.path.join(feature_path,'data_dropout.h5'),key='data').reset_index()\n",
    "print(data.info())\n",
    "train_len = 1504120\n",
    "#train_len = 2593669\n",
    "#train_len = 1504441 #dropout=1\n",
    "#train_len = 1504630 #dropout=2\n",
    "\n",
    "'''修改'''\n",
    "# # add: user_topic_answer_count\n",
    "# with open(os.path.join(feature_path, 'u_topic_a_zishu_train.pkl'), 'rb') as file:\n",
    "#     train_u_t_c = pickle.load(file)\n",
    "# #print(question_topic.head(5))\n",
    "# with open(os.path.join(feature_path, 'u_topic_a_zishu_val.pkl'), 'rb') as file:\n",
    "#     test_u_t_c = pickle.load(file)\n",
    "# train_utc = pd.merge(data[:train_len], train_u_t_c, on='uid', how='left')\n",
    "# test_utc = pd.merge(data[train_len:], test_u_t_c, on='uid', how='left')\n",
    "# data = pd.concat([train_utc, test_utc], axis=0, sort=True)  \n",
    "# del train_utc, test_utc\n",
    "\n",
    "# add: user_topic_answer_count\n",
    "with open(os.path.join(feature_path, 'user_topic_all_count_train.pkl'), 'rb') as file:\n",
    "    train_u_t_c = pickle.load(file)[['uid', 'u_t_a_count_sum', 'u_t_a_count_mean','u_t_a_num', 'u_t_a_count_max', 'u_t_a_count_std', 'u_t_a_count_min']]\n",
    "    train_u_t_c = reduce_mem_usage(train_u_t_c)\n",
    "#print(question_topic.head(5))\n",
    "with open(os.path.join(feature_path, 'user_topic_all_count_test.pkl'), 'rb') as file:\n",
    "    test_u_t_c = pickle.load(file)[['uid', 'u_t_a_count_sum', 'u_t_a_count_mean','u_t_a_num', 'u_t_a_count_max', 'u_t_a_count_std', 'u_t_a_count_min']]\n",
    "    test_u_t_c = reduce_mem_usage(test_u_t_c)\n",
    "train_utc = pd.merge(data[:train_len], train_u_t_c, on='uid', how='left')\n",
    "test_utc = pd.merge(data[train_len:], test_u_t_c, on='uid', how='left')\n",
    "data = pd.concat([train_utc, test_utc], axis=0, sort=True)  \n",
    "data['u_t_a_q_prt'] = data['u_t_a_num'] / data['u_ans_count']\n",
    "del train_utc, test_utc, train_u_t_c, test_u_t_c\n",
    "print(\"user topic loaded\")\n",
    "\n",
    "# add: question_topic_answer_count\n",
    "with open(os.path.join(feature_path, 'que_topic_all_count_train.pkl'), 'rb') as file:\n",
    "    train_q_t_c = pickle.load(file)#'q_t_a_inv_mean_mean', 'q_t_a_inv_mean_max', 'q_t_a_inv_mean_min', 'q_t_a_inv_mean_std',]]#['uid', 'u_t_a_count_sum', 'u_t_a_count_mean', 'u_t_a_count_max', 'u_t_a_count_std', 'u_t_a_count_min']\n",
    "    #train_q_t_c = reduce_mem_usage(train_q_t_c)\n",
    "#print(question_topic.head(5))\n",
    "with open(os.path.join(feature_path, 'que_topic_all_count_val.pkl'), 'rb') as file:\n",
    "    test_q_t_c = pickle.load(file)#'q_t_a_inv_mean_mean', 'q_t_a_inv_mean_max', 'q_t_a_inv_mean_min', 'q_t_a_inv_mean_std',]]#['uid', 'u_t_a_count_sum', 'u_t_a_count_mean', 'u_t_a_count_max', 'u_t_a_count_std', 'u_t_a_count_min']\n",
    "    #test_q_t_c = reduce_mem_usage(test_q_t_c)\n",
    "train_qtc = pd.merge(data[:train_len], train_q_t_c, left_on='qid', right_on='问题id', how='left').drop(['问题id'], axis=1, inplace=False)\n",
    "test_qtc = pd.merge(data[train_len:], test_q_t_c, left_on='qid', right_on='问题id', how='left').drop(['问题id'], axis=1, inplace=False)\n",
    "data = pd.concat([train_qtc, test_qtc], axis=0, sort=True)  \n",
    "del train_qtc, test_qtc, train_q_t_c, test_q_t_c\n",
    "print(\"question topic loaded\")\n",
    "\n",
    "\n",
    "# add: question_basic_extra_feats\n",
    "question_info1 = pd.read_hdf(os.path.join(feature_path,'question_info1.hd5'), key='data')\n",
    "data = pd.merge(data, question_info1, left_on='qid', right_on='问题id', how='left').drop('问题id', axis=1)\n",
    "data['diff_qi_days'] = data['day'] - data['q_day']\n",
    "data['diff_qi_hours'] = data['hour'] - data['q_hour']\n",
    "data.loc[:,'diff_qi_hours'] += 23\n",
    "drop_feat = ['q_day','q_hour']\n",
    "data  = data.drop(drop_feat, axis=1)\n",
    "print(data.info())\n",
    "\n",
    "# add: user_question_topic_embedding_similarity_feats\n",
    "with open(os.path.join(feature_path,'test_cosine_similarity_without_5_10_drop_embedding.pkl'), 'rb') as file:\n",
    "    test_cosine_sim = pickle.load(file).drop_duplicates(subset=['uid','qid'], keep='first', inplace=False)\n",
    "with open(os.path.join(feature_path,'train_cosine_similarity_without_5_10_drop_embedding.pkl'), 'rb') as file:\n",
    "    train_cosine_sim = pickle.load(file).drop_duplicates(subset=['uid','qid'], keep='first', inplace=False)\n",
    "train_sim = pd.merge(data[:train_len], train_cosine_sim, on=['uid','qid'], how='left')\n",
    "test_sim = pd.merge(data[train_len:], test_cosine_sim, on=['uid','qid'], how='left')\n",
    "data = pd.concat([train_sim, test_sim], axis=0, sort=True)  \n",
    "del train_sim, test_sim\n",
    "\n",
    "# #add: xgb_score\n",
    "# xgb_results = pd.read_csv('result_xgb.txt', sep='\\t')\n",
    "# xgb_results.columns = ['qid','uid','i_time','xgb_score']\n",
    "# xgb_results['day'] = xgb_results['i_time'].apply(lambda x:x.split('-')[0].split('D')[1]).astype(int)\n",
    "# xgb_results['hour'] = xgb_results['i_time'].apply(lambda x:x.split('-')[1].split('H')[1]).astype(int)\n",
    "# del xgb_results['i_time']\n",
    "# xgb_results = xgb_results.drop_duplicates(subset=['uid','qid','day','hour'], keep='first', inplace=False)\n",
    "# data = pd.merge(data, xgb_results, on=['uid','qid','day','hour'], how='left')\n",
    "\n",
    "# #add: xgb_score\n",
    "# lgb1_results = pd.read_csv('resultlgbm.txt', sep='\\t')\n",
    "# lgb1_results.columns = ['uid','qid','i_time','lgb1_score']\n",
    "# lgb1_results['day'] = lgb1_results['i_time'].apply(lambda x:x.split('-')[0].split('D')[1]).astype(int)\n",
    "# lgb1_results['hour'] = lgb1_results['i_time'].apply(lambda x:x.split('-')[1].split('H')[1]).astype(int)\n",
    "# del lgb1_results['i_time']\n",
    "# lgb1_results = lgb1_results.drop_duplicates(subset=['uid','qid','day','hour'], keep='first', inplace=False)\n",
    "# data = pd.merge(data, lgb1_results, on=['uid','qid','day','hour'], how='left')\n",
    "\n",
    "# #add: dnn_score_output\n",
    "\n",
    "# dnn_results_train = pd.read_csv('submit_train_all1.txt', sep='\\t')\n",
    "# dnn_results_train.columns = ['qid','uid','i_time','dnn_score']\n",
    "# dnn_results_test = pd.read_csv('submit_test_all2.txt', sep='\\t')\n",
    "# dnn_results_test.columns = ['qid','uid','i_time','dnn_score']\n",
    "# dnn_results = pd.concat([dnn_results_train, dnn_results_test], axis=0, sort=True)\n",
    "\n",
    "# dnn_results['day'] = dnn_results['i_time'].apply(lambda x:x.split('-')[0].split('D')[1]).astype(int)\n",
    "# dnn_results['hour'] = dnn_results['i_time'].apply(lambda x:x.split('-')[1].split('H')[1]).astype(int)\n",
    "# del dnn_results['i_time']\n",
    "# dnn_results = dnn_results.drop_duplicates(subset=['uid','qid','day','hour'], keep='first', inplace=False)\n",
    "# data = pd.merge(data, dnn_results, on=['uid','qid','day','hour'], how='left')\n",
    "\n",
    "# add: user_answer_side_feats\n",
    "user_answer_feats_val = pd.read_hdf(os.path.join(feature_path,'user_answer_val.h5'),key='data')\n",
    "user_answer_feats_train = pd.read_hdf(os.path.join(feature_path,'user_answer_train.h5'),key='data')\n",
    "# user_answer_feats_train = user_answer_feats_train[['用户id','u_a_i_diffhour_mean','u_a_i_diffhour_sum',\n",
    "#                                                    'u_a_i_diffhour_max','u_a_i_diffhour_min','u_a_i_diffday_mean',\n",
    "#                                                    'u_a_i_diffday_sum','u_a_i_diffday_max','u_a_i_diffday_min',\n",
    "#                                                    '用户习惯回答时间-hour','用户最近回答数-3天','用户最近回答数-7天','用户最近回答数-14天',\n",
    "#                                                    '用户习惯回答时间-wkday']]#,'用户上次回答时间-day']]#,\n",
    "# user_answer_feats_val = user_answer_feats_val[['用户id','u_a_i_diffhour_mean','u_a_i_diffhour_sum',\n",
    "#                                                    'u_a_i_diffhour_max','u_a_i_diffhour_min','u_a_i_diffday_mean',\n",
    "#                                                    'u_a_i_diffday_sum','u_a_i_diffday_max','u_a_i_diffday_min',\n",
    "#                                                    '用户习惯回答时间-hour','用户最近回答数-3天','用户最近回答数-7天','用户最近回答数-14天',\n",
    "#                                               '用户习惯回答时间-wkday']]#,'用户上次回答时间-day']]#' ,\n",
    "\n",
    "# user_answer_feats_train = user_answer_feats_train.rename(columns={'用户习惯回答时间-hour':'u_most_a_hour','用户最近回答数-3天':'u_a_num_latest3',\n",
    "#                                         '用户最近回答数-7天':'u_a_num_latest7','用户最近回答数-14天':'u_a_num_latest14',\n",
    "#                                         '用户习惯回答时间-wkday':'u_most_a_wkday','用户上次回答时间-day':'u_last_a_time'})#,\n",
    "# user_answer_feats_val = user_answer_feats_val.rename(columns={'用户习惯回答时间-hour':'u_most_a_hour','用户最近回答数-3天':'u_a_num_latest3',\n",
    "#                                         '用户最近回答数-7天':'u_a_num_latest7','用户最近回答数-14天':'u_a_num_latest14',\n",
    "#                                         '用户习惯回答时间-wkday':'u_most_a_wkday','用户上次回答时间-day':'u_last_a_time'})#\n",
    "\n",
    "#print(user_answer_feats_train[['用户id','u_last_a_time']])\n",
    "\n",
    "train2 = pd.merge(data[:train_len], user_answer_feats_train, left_on='uid', right_on='用户id', how='left' ).drop('用户id',axis=1)\n",
    "test2 = pd.merge(data[train_len:],user_answer_feats_val, left_on='uid', right_on='用户id', how='left' ).drop('用户id',axis=1)\n",
    "train2['u_a_last_daytime'] = train2['day'] - train2['u_a_last_daytime']\n",
    "test2['u_a_last_daytime'] = test2['day'] - test2['u_a_last_daytime']\n",
    "\n",
    "data = pd.concat([train2, test2], axis=0, sort=True)\n",
    "del train2, test2\n",
    "\n",
    "# add: user_topic_side_feats\n",
    "user_topic_feats = pd.read_hdf(os.path.join(feature_path,'user_topic_feat.h5'),key='data')\n",
    "data = pd.merge(data,user_topic_feats, left_on='uid', right_on='author_id',how='left').drop('author_id',axis=1)\n",
    "print(\"loaded topic feats\")\n",
    "\n",
    "user_question_topic = pd.read_hdf(os.path.join(feature_path,'member_question_feat.h5'), key='data')\n",
    "user_question_topic = user_question_topic.rename(columns={'用户问题的话题关注重叠数':'u_q_topic_att_num','用户问题的话题感兴趣重叠数':'u_q_topic_ints_num'})\n",
    "\n",
    "data = pd.merge(data, user_question_topic, left_on=['uid','qid'], right_on=['author_id','question_id'], how='left').drop(['author_id','question_id'],axis=1)\n",
    "print(\"loaded user_question topics\")\n",
    "\n",
    "# add: question_side_feats\n",
    "# user_qa_days_feats_train = pd.read_hdf(os.path.join(feature_path,'../u_qa_day_feats_train.h5'),key='data')\n",
    "# user_qa_days_feats_val = pd.read_hdf(os.path.join(feature_path,'../u_qa_day_feats_val.h5'),key='data')\n",
    "# print(\"loaded u_qa_days\")\n",
    "\n",
    "'''修改end'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns.values)\n",
    "print(len(data.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_count1(data, count_fea):\n",
    "    for feat in count_fea: \n",
    "        data[feat] = data[feat].fillna(0)\n",
    "        data[feat] = (data[feat] - data[feat].min()) / (np.percentile(np.array(data[feat]), 90) - data[feat].min())\n",
    "    return data\n",
    "\n",
    "data1 = basic_count1(data, ['u_inv_count','q_inv_count',\n",
    "                            'q_ans_count', 'u_q_topic_att_num', 'u_q_topic_ints_num', \n",
    "                           'u_ans_count','q_title_num', 'u_a_num_last14','u_a_num','u_a_num_last3','num_atten_topic', 'num_interest_topic',\n",
    "                           'u_q_topic_att_num', 'u_q_topic_ints_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_count2(data, count_fea):\n",
    "    for feat in count_fea: \n",
    "        data[feat] = data[feat].fillna(0)\n",
    "        data[feat] = np.log10(data[feat])\n",
    "    return data\n",
    "\n",
    "data1 = basic_count1(data1, ['q_inv_std','q_t_a_count_std','q_t_a_count_std',\n",
    "                           'q_t_a_inv_mean_std', 'q_t_a_mark_count_std','q_t_a_neg_inv_mean_std',\n",
    "                           'q_t_a_thank_count_std','u_inv_std','std_interest_values','min_interest_values', 'max_interest_values', 'mean_interest_values',\n",
    "                            'q_t_a_inv_count_mean', 'q_t_a_inv_mean_max', 'q_t_a_inv_mean_mean', 'q_t_a_inv_mean_min','q_t_a_inv_std_mean', 'q_t_a_inv_sum_mean',\n",
    "                            'u_inv_mean', \n",
    "                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [x for x in data.columns if x not in ('u_ans_count','dnn_score','u_a_last_daytime','xgb_score', 'lgb1_score','index','uid', 'qid','dropoutrate', 'label', 'dt', 'day', )]#'q_inv_mean', 'q_inv_sum', 'q_inv_std', 'q_inv_count')]\n",
    "#feature_cols = [x for x in feature_cols if x not in ('u_a_mean_hour_last1week', 'u_a_mean_hour_last2week', 'u_a_mean_hour_last3day', 'u_a_most_hour', 'u_a_most_hour_last1week', 'u_a_most_hour_last2week', 'u_a_most_hour_last3day', 'u_a_most_wkday', 'u_a_num', 'u_a_num_last14', 'u_a_num_last3',)]#'q_inv_mean', 'q_inv_sum', 'q_inv_std', 'q_inv_count')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(feature_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "220"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ------------------------------**分支1：整体训练**---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all = data.iloc[train_len:]\n",
    "assert len(test_all) == test1.shape[0]\n",
    "train_all = data.iloc[:train_len]\n",
    "logging.info(\"train shape %d, test shape %s\", train_len, test_all.shape)\n",
    "\n",
    "sub = test1.copy().reset_index()\n",
    "\n",
    "result = train_for4(train_all, test_all, 'ouoq', sub, feature_cols)\n",
    "result = result.drop(['index'], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.head())\n",
    "result.to_csv(os.path.join(result_path, 'result_withouttime222.txt'), index=None, header=None, sep='\\t')\n",
    "print(\"result written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制重要性因子的图表\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "lgbm111 = lgb.Booster(model_file='model_saved/model.txt')\n",
    "plt.figure(figsize=(2400,1200))\n",
    "lgb.plot_importance(lgbm111, max_num_features=30)\n",
    "plt.title(\"Featurertances\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ------------------------------**分支1：整体训练结束**-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -----------------------**分支2：分类训练**---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data1.head(3))\n",
    "\n",
    "test_all = data1.iloc[train_len:]\n",
    "print(len(test_all))\n",
    "assert len(test_all) == test1.shape[0]\n",
    "train_all = data1.iloc[:train_len]\n",
    "logging.info(\"train shape %d, test shape %s\", train_len, test_all.shape)\n",
    "\n",
    "train_old_users, train_new_users = get_new_users(train_all, answer_info, inv_info, 3860)\n",
    "test_old_users, test_new_users = get_new_users(test_all, answer_info, inv_info, 3868)\n",
    "train_old_question, train_new_question = get_new_quest(train_all, answer_info, inv_info, 3860)\n",
    "test_old_question, test_new_question = get_new_quest(test_all, answer_info, inv_info, 3868)\n",
    "\n",
    "train_ouoq = train_all[train_all['uid'].isin(train_old_users)][train_all['qid'].isin(train_old_question)]\n",
    "train_nuoq = train_all[train_all['uid'].isin(train_new_users)][train_all['qid'].isin(train_old_question)]\n",
    "train_ounq = train_all[train_all['uid'].isin(train_old_users)][train_all['qid'].isin(train_new_question)]\n",
    "train_nunq = train_all[train_all['uid'].isin(train_new_users)][train_all['qid'].isin(train_new_question)]\n",
    "\n",
    "# train_oq = train_all[train_all['qid'].isin(train_old_question)]\n",
    "# train_nq = train_all[train_all['qid'].isin(train_new_question)]\n",
    "\n",
    "test_ouoq = test_all[test_all['uid'].isin(test_old_users)][test_all['qid'].isin(test_old_question)]\n",
    "test_nuoq = test_all[test_all['uid'].isin(test_new_users)][test_all['qid'].isin(test_old_question)]\n",
    "test_nunq = test_all[test_all['uid'].isin(test_new_users)][test_all['qid'].isin(test_new_question)]\n",
    "test_ounq = test_all[test_all['uid'].isin(test_old_users)][test_all['qid'].isin(test_new_question)]\n",
    "\n",
    "# test_oq = test_all[test_all['qid'].isin(test_old_question)]\n",
    "# test_nq = test_all[test_all['qid'].isin(test_new_question)]\n",
    "\n",
    "sub = test1.copy().reset_index()\n",
    "#sub_size = len(sub)\n",
    "sub['index1'] = sub.index\n",
    "print(sub.info())\n",
    "\n",
    "sub_ouoq = sub[sub['uid'].isin(test_old_users)][sub['qid'].isin(test_old_question)]\n",
    "sub_ounq = sub[sub['uid'].isin(test_old_users)][sub['qid'].isin(test_new_question)]\n",
    "sub_nuoq = sub[sub['uid'].isin(test_new_users)][sub['qid'].isin(test_old_question)]\n",
    "sub_nunq = sub[sub['uid'].isin(test_new_users)][sub['qid'].isin(test_new_question)]\n",
    "\n",
    "# sub_oq = sub[sub['qid'].isin(test_old_question)]\n",
    "# sub_nq = sub[sub['qid'].isin(test_new_question)]\n",
    "\n",
    "# sub.to_csv('result.txt', index=None, header=None, sep='\\t')\n",
    "\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_old_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_all.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols1 = [x for x in data.columns if x not in ('u_a_last_daytime', 'xgb_score', 'lgb1_score','index','uid', 'qid','dropoutrate', 'label', 'dt', 'day', )]#'q_inv_mean', 'q_inv_sum', 'q_inv_std', 'q_inv_count')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_oq = train_for4(train_all, test_oq, 'ouoq', sub_oq, feature_cols1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_nq = train_for4(train_all, test_nq, 'ounq', sub_nq, feature_cols1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_result = pd.concat([sub_oq, sub_nq], axis=0)  \n",
    "sub_result.sort_values(by=\"index1\" , ascending=True, inplace=True)\n",
    "print(sub_result.head(15))\n",
    "sub_result2 = sub_result.drop(['index','index1'], axis=1)\n",
    "print(sub_result2.head(5))\n",
    "\n",
    "sub_result2.to_csv('result.txt', index=None, header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_ouoq[train_ouoq['xgb_score']!=None]['xgb_score'].head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------------------分隔线----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ouoq = train_for4(train_all, test_ouoq, 'ouoq', sub_ouoq, feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ounq = train_for4(train_all, test_ounq, 'ounq', sub_ounq, feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_nuoq = train_for4(train_all, test_nuoq, 'nuoq', sub_nuoq, feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_nunq = train_for4(train_all, test_nunq, 'nunq', sub_nunq, feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_result = pd.concat([sub_ouoq, sub_nuoq], axis=0)  \n",
    "sub_result = pd.concat([sub_result, sub_ounq], axis=0)  \n",
    "sub_result = pd.concat([sub_result, sub_nunq], axis=0)  \n",
    "sub_result.sort_values(by=\"index1\" , ascending=True, inplace=True)\n",
    "print(sub_result.head(15))\n",
    "sub_result2 = sub_result.drop(['index','index1'], axis=1)\n",
    "print(sub_result2.head(5))\n",
    "\n",
    "sub_result2.to_csv('result.txt', index=None, header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --------------------------------------**分支2：分类训练结束**-------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
