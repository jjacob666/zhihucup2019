{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "import logging\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "data_path = '../../data'\n",
    "#train1 = pd.read_csv(os.path.join(data_path, 'invite_info_0926.txt'), header=None, sep='\\t')\n",
    "test1 = pd.read_csv(os.path.join(data_path, 'invite_info_evaluate_1_0926.txt'), header=None, sep='\\t')\n",
    "data = pd.read_hdf('data_dropout.h5',key='data').reset_index()\n",
    "print(data.info())\n",
    "\n",
    "\n",
    "'''修改'''\n",
    "\n",
    "# add: question_basic_extra_feats\n",
    "# question_info1 = pd.read_hdf('question_info1.hd5', key='data')\n",
    "# data = pd.merge(data, question_info1, left_on='qid', right_on='问题id', how='left').drop('问题id', axis=1)\n",
    "# data['diff_qi_days'] = data['day'] - data['q_day']\n",
    "# data['diff_qi_hours'] = data['hour'] - data['q_hour']\n",
    "# data.loc[:,'diff_qi_hours'] += 23\n",
    "# drop_feat = ['q_day','q_hour']\n",
    "# data  = data.drop(drop_feat, axis=1)\n",
    "# print(data.info())\n",
    "\n",
    "# add: user_question_topic_embedding_similarity_feats\n",
    "import pickle\n",
    "with open('test_uid_qid_cosine_sim.pkl', 'rb') as file:\n",
    "    test_cosine_sim = pickle.load(file).drop_duplicates(subset=['uid','qid'], keep='first', inplace=False)\n",
    "with open('train_label_uid_qid_cosine_sim.pkl', 'rb') as file:\n",
    "    train_cosine_sim = pickle.load(file).drop_duplicates(subset=['uid','qid'], keep='first', inplace=False)\n",
    "train_sim = pd.merge(data[:1506141], train_cosine_sim, on=['uid','qid'], how='left')\n",
    "test_sim = pd.merge(data[1506141:], test_cosine_sim, on=['uid','qid'], how='left')\n",
    "data = pd.concat([train_sim, test_sim], axis=0, sort=True)  \n",
    "del train_sim, test_sim\n",
    "\n",
    "#add: dnn_score_output\n",
    "# dnn_results = pd.read_csv('submit_auto_drop.txt', sep='\\t')\n",
    "# dnn_results.columns = ['qid','uid','i_time','dnn_score']\n",
    "# dnn_results['day'] = dnn_results['i_time'].apply(lambda x:x.split('-')[0].split('D')[1]).astype(int)\n",
    "# dnn_results['hour'] = dnn_results['i_time'].apply(lambda x:x.split('-')[1].split('H')[1]).astype(int)\n",
    "# del dnn_results['i_time']\n",
    "# dnn_results = dnn_results.drop_duplicates(subset=['uid','qid','day','hour'], keep='first', inplace=False)\n",
    "# data = pd.merge(data, dnn_results, on=['uid','qid','day','hour'], how='left')\n",
    "\n",
    "# add: user_answer_side_feats\n",
    "user_answer_feats_val = pd.read_hdf('../user_answer_val.h5',key='data')\n",
    "user_answer_feats_train = pd.read_hdf('../user_answer_train.h5',key='data')\n",
    "user_answer_feats_train = user_answer_feats_train[['用户id','u_a_i_diffhour_mean','u_a_i_diffhour_sum',\n",
    "                                                   'u_a_i_diffhour_max','u_a_i_diffhour_min','u_a_i_diffday_mean',\n",
    "                                                   'u_a_i_diffday_sum','u_a_i_diffday_max','u_a_i_diffday_min',\n",
    "                                                   '用户习惯回答时间-hour','用户最近回答数-3天','用户最近回答数-7天','用户最近回答数-14天',\n",
    "                                                   '用户习惯回答时间-wkday']]#,'用户上次回答时间-day']]#,\n",
    "user_answer_feats_val = user_answer_feats_val[['用户id','u_a_i_diffhour_mean','u_a_i_diffhour_sum',\n",
    "                                                   'u_a_i_diffhour_max','u_a_i_diffhour_min','u_a_i_diffday_mean',\n",
    "                                                   'u_a_i_diffday_sum','u_a_i_diffday_max','u_a_i_diffday_min',\n",
    "                                                   '用户习惯回答时间-hour','用户最近回答数-3天','用户最近回答数-7天','用户最近回答数-14天',\n",
    "                                              '用户习惯回答时间-wkday']]#,'用户上次回答时间-day']]#' ,\n",
    "\n",
    "user_answer_feats_train = user_answer_feats_train.rename(columns={'用户习惯回答时间-hour':'u_most_a_hour','用户最近回答数-3天':'u_a_num_latest3',\n",
    "                                        '用户最近回答数-7天':'u_a_num_latest7','用户最近回答数-14天':'u_a_num_latest14',\n",
    "                                        '用户习惯回答时间-wkday':'u_most_a_wkday','用户上次回答时间-day':'u_last_a_time'})#,\n",
    "user_answer_feats_val = user_answer_feats_val.rename(columns={'用户习惯回答时间-hour':'u_most_a_hour','用户最近回答数-3天':'u_a_num_latest3',\n",
    "                                        '用户最近回答数-7天':'u_a_num_latest7','用户最近回答数-14天':'u_a_num_latest14',\n",
    "                                        '用户习惯回答时间-wkday':'u_most_a_wkday','用户上次回答时间-day':'u_last_a_time'})#\n",
    "\n",
    "#print(user_answer_feats_train[['用户id','u_last_a_time']])\n",
    "\n",
    "train2 = pd.merge(data[:1506141], user_answer_feats_train, left_on='uid', right_on='用户id', how='left' ).drop('用户id',axis=1)\n",
    "test2 = pd.merge(data[1506141:],user_answer_feats_val, left_on='uid', right_on='用户id', how='left' ).drop('用户id',axis=1)\n",
    "#train2['u_last_a_time'] = train2['day'] - train2['u_last_a_time']\n",
    "#test2['u_last_a_time'] = test2['day'] - test2['u_last_a_time']\n",
    "\n",
    "data = pd.concat([train2, test2], axis=0, sort=True)\n",
    "del train2, test2\n",
    "\n",
    "# add: user_topic_side_feats\n",
    "user_topic_feats = pd.read_hdf('../user_topic_feat.h5',key='data')\n",
    "data = pd.merge(data,user_topic_feats, left_on='uid', right_on='author_id',how='left').drop('author_id',axis=1)\n",
    "print(\"loaded topic feats\")\n",
    "\n",
    "user_question_topic = pd.read_hdf('../member_question_feat.h5', key='data')\n",
    "user_question_topic = user_question_topic.rename(columns={'用户问题的话题关注重叠数':'u_q_topic_att_num','用户问题的话题感兴趣重叠数':'u_q_topic_ints_num'})\n",
    "\n",
    "data = pd.merge(data, user_question_topic, left_on=['uid','qid'], right_on=['author_id','question_id'], how='left').drop(['author_id','question_id'],axis=1)\n",
    "print(\"loaded user_question topics\")\n",
    "#user_qa_days_feats_train = pd.read_hdf('../u_qa_day_feats_train.h5',key='data')\n",
    "#user_qa_days_feats_val = pd.read_hdf('../u_qa_day_feats_val.h5',key='data')\n",
    "#print(\"loaded u_qa_days\")\n",
    "\n",
    "'''修改end'''\n",
    "\n",
    "print(data.head(3))\n",
    "\n",
    "'''mmm'''\n",
    "feature_cols = [x for x in data.columns if x not in ('index','uid', 'qid','dropoutrate', 'label', 'dt', 'day', 'q_inv_mean', 'q_inv_sum', 'q_inv_std', 'q_inv_count')]\n",
    "'''end'''\n",
    "\n",
    "print(feature_cols)\n",
    "\n",
    "\n",
    "# #对负样本采样\n",
    "# train_all_pos = train_all[train_all['label']==1]\n",
    "# train_all_neg = train_all[train_all['label']==0].sample(frac=0.5)\n",
    "# train_all = shuffle(pd.concat([train_all_pos, train_all_neg], axis=0, sort=True))\n",
    "# print(\"shuffled\")\n",
    "# del train_all_pos, train_all_neg\n",
    "\n",
    "train_all = data.iloc[:1506141]\n",
    "train_all = shuffle(train_all)\n",
    "X_train_all = train_all[feature_cols]\n",
    "y_train_all = train_all['label']\n",
    "test = data.iloc[1506141:]\n",
    "\n",
    "\n",
    "del data\n",
    "print(len(test))\n",
    "assert len(test) == test1.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logging.info(\"train shape %d, test shape %s\", 1506141, test.shape)\n",
    "\n",
    "fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores=[]\n",
    "\n",
    "for index, (train_idx, val_idx) in enumerate(fold.split(X=X_train_all, y=y_train_all)):\n",
    "    break\n",
    "X_train, X_val, y_train, y_val = X_train_all.iloc[train_idx][feature_cols], X_train_all.iloc[val_idx][feature_cols], \\\n",
    "                                 y_train_all.iloc[train_idx], \\\n",
    "                                 y_train_all.iloc[val_idx]\n",
    "\n",
    "\n",
    "model_xgb = xgb.XGBClassifier(max_depth=6, n_estimators=2000, seed=1000, silent=True)\n",
    "\n",
    "    \n",
    "model_xgb.fit(X_train, y_train,\n",
    "              eval_metric='auc',\n",
    "              eval_set=[(X_val, y_val)],\n",
    "              verbose=True,\n",
    "              early_stopping_rounds=1000)\n",
    "    \n",
    "    \n",
    "#     score = model_xgb.best_score_['eval']['auc']\n",
    "#     scores.append(score)\n",
    "#     print('fold %d round %d : score: %.6f | mean score %.6f' % (index+1, model_xgb.best_iteration_, score, np.mean(scores))) \n",
    "   \n",
    "#     y_pred = model_xgb.predict_proba(test[feature_cols])[:,1]\n",
    "#     np.savetxt('y_pred%d.txt'%(index+1),y_pred)\n",
    "#     print('saved')\n",
    "\n",
    "del X_train_all\n",
    "\n",
    "sub = test1.copy()\n",
    "sub_size = len(sub)\n",
    "sub['label'] = model_xgb.predict_proba(test[feature_cols])[:, 1]\n",
    "\n",
    "\n",
    "sub.to_csv('result_xgb.txt', index=None, header=None, sep='\\t')\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(pd.DataFrame({\n",
    "    'column': feature_cols,\n",
    "    'importance': model_xgb.feature_importances_\n",
    "}).sort_values(by='importance', ascending=False))\n",
    "\n",
    "model_xgb.save_model('xgb_model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = pd.read_csv(os.path.join(data_path, 'invite_info_evaluate_1_0926.txt'), header=None, sep='\\t')\n",
    "data = pd.read_hdf('data_dropout.h5',key='data').reset_index()\n",
    "sub = pd.concat(data, test1, axis=0)\n",
    "sub_size = len(sub)\n",
    "sub['label'] = model_xgb.predict_proba(test[feature_cols])[:, 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
